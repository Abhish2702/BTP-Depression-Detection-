{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtdvicslpqiUygiBQtm0Oj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhish2702/BTP-Depression-Detection-/blob/main/btp_using_fasttext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M7qKxdrZnT5",
        "outputId": "b66d29b8-733a-4dca-8a18-6fadd22118ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_tweets_dataset=pd.read_csv(\"/content/clean_d_tweets.csv\")\n",
        "non_d_tweets_dataset=pd.read_csv(\"/content/clean_non_d_tweets.csv\")\n",
        "d_tweets_dataset.tweet = d_tweets_dataset.tweet.fillna('')\n",
        "non_d_tweets_dataset.tweet = non_d_tweets_dataset.tweet.fillna('')"
      ],
      "metadata": {
        "id": "OJK8TevKZuCI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_tweets=d_tweets_dataset['tweet']"
      ],
      "metadata": {
        "id": "jIiLlB5cZ2J7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_d_tweets=non_d_tweets_dataset['tweet']"
      ],
      "metadata": {
        "id": "G4odwZIBZ4QP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y1=np.ones((3082,1),dtype=\"int\")"
      ],
      "metadata": {
        "id": "Z4xFPGHdZ6sU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y2=np.zeros((2505,1),dtype=\"int\")"
      ],
      "metadata": {
        "id": "zL7nDzNAZ8-7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=np.concatenate((y1,y2),axis=0)"
      ],
      "metadata": {
        "id": "-j1Szd2kaAUp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res=pd.DataFrame(y)"
      ],
      "metadata": {
        "id": "71EjJw9BaE_n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt=pd.concat([d_tweets,non_d_tweets],axis=0)"
      ],
      "metadata": {
        "id": "-Xd8EF9_aHoU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt.shape\n",
        "txt=txt.reset_index(drop=\"true\")"
      ],
      "metadata": {
        "id": "ZTUg-bEtaJ9B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.concat([txt,res],axis=1,join=\"inner\")"
      ],
      "metadata": {
        "id": "jyxU_c9daQBl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns=[\"text\",\"result\"]\n"
      ],
      "metadata": {
        "id": "GXnucdd2aSuq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "RYMhNHCkaVe7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = [word_tokenize(text.lower()) for text in df['text']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "CNSuS8NNaemX",
        "outputId": "733b69be-3343-4e11-97fa-d547fe181ec4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-4b0d69d32fd1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "# Train FastText model\n",
        "fasttext_model = FastText(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n"
      ],
      "metadata": {
        "id": "PXl5De4SaqcQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the FastText word vectors\n",
        "word_vectors = fasttext_model.wv\n"
      ],
      "metadata": {
        "id": "KAXqjWHcavMJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to FastText word embeddings\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokenized_text)\n",
        "X_sequences = tokenizer.texts_to_sequences(tokenized_text)\n",
        "X_padded = pad_sequences(X_sequences, maxlen=100)\n",
        "\n",
        "# X_fasttext_padded = pad_sequences(X_fasttext, maxlen=100)\n",
        "# X_fasttext_padded\n",
        "# y = df['result'].values\n"
      ],
      "metadata": {
        "id": "S2WfmrR_azjJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare target variable y (same as before)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "I4bDh90rbUr7"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_fasttext = []\n",
        "for sentence in tokenized_text:\n",
        "    embedding = []\n",
        "    for word in sentence:\n",
        "        if word in word_vectors:\n",
        "            embedding.append(word_vectors[word])\n",
        "    X_fasttext.append(embedding)\n",
        "\n",
        "# Find the maximum sequence length in X_fasttext\n",
        "max_sequence_length = max(len(embedding) for embedding in X_fasttext)\n",
        "\n",
        "# Pad sequences to the maximum length\n",
        "X_fasttext_padded = pad_sequences(X_fasttext, maxlen=100, dtype='float32', padding='post')\n",
        "X_fasttext_padded.shape\n",
        "y = df['result'].values"
      ],
      "metadata": {
        "id": "-LzqVWYJbbFN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_fasttext_padded, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "beRS6NGCem6T"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM-RNN Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2, input_shape=(100, 100)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Step 7: Train the Model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Step 8: Evaluate the Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Step 9: Make Predictions (same as before)\n"
      ],
      "metadata": {
        "id": "q3sFCIDMe1cY",
        "outputId": "2dd37879-6e32-40b8-fceb-c0206aea2ec6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "140/140 [==============================] - 24s 154ms/step - loss: 0.6883 - accuracy: 0.5525 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 21s 150ms/step - loss: 0.6887 - accuracy: 0.5547 - val_loss: 0.6894 - val_accuracy: 0.5456\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 21s 150ms/step - loss: 0.6878 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 22s 155ms/step - loss: 0.6865 - accuracy: 0.5572 - val_loss: 0.6878 - val_accuracy: 0.5519\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 21s 151ms/step - loss: 0.6746 - accuracy: 0.5831 - val_loss: 0.6740 - val_accuracy: 0.6109\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 21s 151ms/step - loss: 0.5918 - accuracy: 0.6811 - val_loss: 0.5158 - val_accuracy: 0.7585\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 21s 150ms/step - loss: 0.5542 - accuracy: 0.7308 - val_loss: 0.5385 - val_accuracy: 0.7513\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 21s 149ms/step - loss: 0.5363 - accuracy: 0.7353 - val_loss: 0.5134 - val_accuracy: 0.7603\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 22s 156ms/step - loss: 0.5394 - accuracy: 0.7360 - val_loss: 0.5169 - val_accuracy: 0.7594\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 21s 151ms/step - loss: 0.5201 - accuracy: 0.7456 - val_loss: 0.5143 - val_accuracy: 0.7424\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 0.5143 - accuracy: 0.7424\n",
            "Accuracy: 74.24%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tDm_EvL7e5ei"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}