{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKSlcP7b068U4oWh/ohReg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhish2702/BTP-Depression-Detection-/blob/main/btp_using_fasttext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M7qKxdrZnT5",
        "outputId": "b66d29b8-733a-4dca-8a18-6fadd22118ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_tweets_dataset=pd.read_csv(\"/content/clean_d_tweets.csv\")\n",
        "non_d_tweets_dataset=pd.read_csv(\"/content/clean_non_d_tweets.csv\")\n",
        "d_tweets_dataset.tweet = d_tweets_dataset.tweet.fillna('')\n",
        "non_d_tweets_dataset.tweet = non_d_tweets_dataset.tweet.fillna('')"
      ],
      "metadata": {
        "id": "OJK8TevKZuCI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_tweets=d_tweets_dataset['tweet']"
      ],
      "metadata": {
        "id": "jIiLlB5cZ2J7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_d_tweets=non_d_tweets_dataset['tweet']"
      ],
      "metadata": {
        "id": "G4odwZIBZ4QP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y1=np.ones((3082,1),dtype=\"int\")"
      ],
      "metadata": {
        "id": "Z4xFPGHdZ6sU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y2=np.zeros((2505,1),dtype=\"int\")"
      ],
      "metadata": {
        "id": "zL7nDzNAZ8-7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=np.concatenate((y1,y2),axis=0)"
      ],
      "metadata": {
        "id": "-j1Szd2kaAUp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res=pd.DataFrame(y)"
      ],
      "metadata": {
        "id": "71EjJw9BaE_n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt=pd.concat([d_tweets,non_d_tweets],axis=0)"
      ],
      "metadata": {
        "id": "-Xd8EF9_aHoU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt.shape\n",
        "txt=txt.reset_index(drop=\"true\")"
      ],
      "metadata": {
        "id": "ZTUg-bEtaJ9B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.concat([txt,res],axis=1,join=\"inner\")"
      ],
      "metadata": {
        "id": "jyxU_c9daQBl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns=[\"text\",\"result\"]\n"
      ],
      "metadata": {
        "id": "GXnucdd2aSuq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "RYMhNHCkaVe7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = [word_tokenize(text.lower()) for text in df['text']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "CNSuS8NNaemX",
        "outputId": "733b69be-3343-4e11-97fa-d547fe181ec4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-4b0d69d32fd1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "# Train FastText model\n",
        "fasttext_model = FastText(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n"
      ],
      "metadata": {
        "id": "PXl5De4SaqcQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the FastText word vectors\n",
        "word_vectors = fasttext_model.wv\n"
      ],
      "metadata": {
        "id": "KAXqjWHcavMJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to FastText word embeddings\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokenized_text)\n",
        "X_sequences = tokenizer.texts_to_sequences(tokenized_text)\n",
        "X_padded = pad_sequences(X_sequences, maxlen=100)\n",
        "\n",
        "# X_fasttext_padded = pad_sequences(X_fasttext, maxlen=100)\n",
        "# X_fasttext_padded\n",
        "# y = df['result'].values\n"
      ],
      "metadata": {
        "id": "S2WfmrR_azjJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare target variable y (same as before)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "I4bDh90rbUr7"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_fasttext = []\n",
        "for sentence in tokenized_text:\n",
        "    embedding = []\n",
        "    for word in sentence:\n",
        "        if word in word_vectors:\n",
        "            embedding.append(word_vectors[word])\n",
        "    X_fasttext.append(embedding)\n",
        "\n",
        "# Find the maximum sequence length in X_fasttext\n",
        "max_sequence_length = max(len(embedding) for embedding in X_fasttext)\n",
        "\n",
        "# Pad sequences to the maximum length\n",
        "X_fasttext_padded = pad_sequences(X_fasttext, maxlen=100, dtype='float32', padding='post')\n",
        "X_fasttext_padded.shape\n",
        "y = df['result'].values"
      ],
      "metadata": {
        "id": "-LzqVWYJbbFN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_fasttext_padded, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "beRS6NGCem6T"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM-RNN Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2, input_shape=(100, 100)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Step 7: Train the Model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Step 8: Evaluate the Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Step 9: Make Predictions (same as before)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3sFCIDMe1cY",
        "outputId": "2dd37879-6e32-40b8-fceb-c0206aea2ec6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "140/140 [==============================] - 24s 154ms/step - loss: 0.6883 - accuracy: 0.5525 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 21s 150ms/step - loss: 0.6887 - accuracy: 0.5547 - val_loss: 0.6894 - val_accuracy: 0.5456\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 21s 150ms/step - loss: 0.6878 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 22s 155ms/step - loss: 0.6865 - accuracy: 0.5572 - val_loss: 0.6878 - val_accuracy: 0.5519\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 21s 151ms/step - loss: 0.6746 - accuracy: 0.5831 - val_loss: 0.6740 - val_accuracy: 0.6109\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 21s 151ms/step - loss: 0.5918 - accuracy: 0.6811 - val_loss: 0.5158 - val_accuracy: 0.7585\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 21s 150ms/step - loss: 0.5542 - accuracy: 0.7308 - val_loss: 0.5385 - val_accuracy: 0.7513\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 21s 149ms/step - loss: 0.5363 - accuracy: 0.7353 - val_loss: 0.5134 - val_accuracy: 0.7603\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 22s 156ms/step - loss: 0.5394 - accuracy: 0.7360 - val_loss: 0.5169 - val_accuracy: 0.7594\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 21s 151ms/step - loss: 0.5201 - accuracy: 0.7456 - val_loss: 0.5143 - val_accuracy: 0.7424\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 0.5143 - accuracy: 0.7424\n",
            "Accuracy: 74.24%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Glove"
      ],
      "metadata": {
        "id": "VNkk9ON0gm34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load pre-trained GloVe embeddings using spaCy (make sure to download the appropriate GloVe model)\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "word_vectors = nlp.vocab\n",
        "\n",
        "# Get word vectors from GloVe\n",
        "def get_word_vector(word):\n",
        "    if word in word_vectors:\n",
        "        return word_vectors[word].vector\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "tDm_EvL7e5ei",
        "outputId": "dfb5a471-4a70-4351-8e59-254703a29c9d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-113200e49fc5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load pre-trained GloVe embeddings using spaCy (make sure to download the appropriate GloVe model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_md\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8iKCNvngvUT",
        "outputId": "5c9112ee-f015-4534-ab3e-7380a05b280f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "zmPB97_kkN9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load pre-trained GloVe embeddings using spaCy (make sure to download the appropriate GloVe model)\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "word_vectors = nlp.vocab\n",
        "\n",
        "# Get word vectors from GloVe\n",
        "def get_word_vector(word):\n",
        "    if word in word_vectors:\n",
        "        return word_vectors[word].vector\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "nUU5rPhJkTfU"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to GloVe word embeddings\n",
        "X_glove = []\n",
        "for sentence in tokenized_text:\n",
        "    embedding = [get_word_vector(word) for word in sentence]\n",
        "    embedding = [word_embedding for word_embedding in embedding if word_embedding is not None]\n",
        "    X_glove.append(embedding)\n",
        "\n",
        "# Find the maximum sequence length in X_glove\n",
        "max_sequence_length = max(len(embedding) for embedding in X_glove)\n",
        "\n",
        "# Pad sequences to the maximum length\n",
        "X_glove_padded = pad_sequences(X_glove, maxlen=max_sequence_length, dtype='float32', padding='post')\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_glove_padded, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "reDC1cuTkiha"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM-RNN Model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2, input_shape=(max_sequence_length, 300)))  # 300 is the dimensionality of GloVe vectors\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Step 4: Train the Model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Step 5: Evaluate the Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# # Step 6: Make Predictions\n",
        "# predictions = model.predict_classes(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRlXCnpnkoxT",
        "outputId": "e37fea2d-b128-4275-8dd5-fbfc627ea266"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "140/140 [==============================] - 10s 56ms/step - loss: 0.5944 - accuracy: 0.6785 - val_loss: 0.5373 - val_accuracy: 0.7191\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 7s 47ms/step - loss: 0.5610 - accuracy: 0.7080 - val_loss: 0.5421 - val_accuracy: 0.7120\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 7s 53ms/step - loss: 0.5524 - accuracy: 0.7169 - val_loss: 0.5271 - val_accuracy: 0.7236\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 7s 54ms/step - loss: 0.5484 - accuracy: 0.7122 - val_loss: 0.5410 - val_accuracy: 0.7299\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 7s 47ms/step - loss: 0.5399 - accuracy: 0.7131 - val_loss: 0.5289 - val_accuracy: 0.7200\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 7s 53ms/step - loss: 0.5338 - accuracy: 0.7299 - val_loss: 0.5379 - val_accuracy: 0.7200\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 7s 47ms/step - loss: 0.5308 - accuracy: 0.7234 - val_loss: 0.5406 - val_accuracy: 0.7182\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 7s 53ms/step - loss: 0.5203 - accuracy: 0.7301 - val_loss: 0.5438 - val_accuracy: 0.7317\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 7s 53ms/step - loss: 0.5170 - accuracy: 0.7346 - val_loss: 0.5370 - val_accuracy: 0.7209\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 7s 47ms/step - loss: 0.5164 - accuracy: 0.7362 - val_loss: 0.5234 - val_accuracy: 0.7308\n",
            "35/35 [==============================] - 0s 10ms/step - loss: 0.5234 - accuracy: 0.7308\n",
            "Accuracy: 73.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yF16UI6bkvFh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}