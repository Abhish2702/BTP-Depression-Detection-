{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZGola02jmXMgVgkkDUiaK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fdfb8c851a2c410497e6cf9b34b45a5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d68eac06b63d489ea47d1c4f868aea25",
              "IPY_MODEL_ca02c6baeeb141879a8efb0d519a25c1",
              "IPY_MODEL_2364cd238c4d4019814753190d176de1"
            ],
            "layout": "IPY_MODEL_7554fdc971d54be5805d74ba29be2563"
          }
        },
        "d68eac06b63d489ea47d1c4f868aea25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcc3b12fc3674882a7c16b01681acc5e",
            "placeholder": "​",
            "style": "IPY_MODEL_7dc70cc552264bc09a28a6871ba4af91",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "ca02c6baeeb141879a8efb0d519a25c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f293f757b5cd4c12ac7119c22c500295",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c52d5d1529424339b2df1a68fdd5b2af",
            "value": 28
          }
        },
        "2364cd238c4d4019814753190d176de1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de09c40752174858b64c17ca4bd3c003",
            "placeholder": "​",
            "style": "IPY_MODEL_2e586aa23375402cbce02c34dae0ab1b",
            "value": " 28.0/28.0 [00:00&lt;00:00, 287B/s]"
          }
        },
        "7554fdc971d54be5805d74ba29be2563": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcc3b12fc3674882a7c16b01681acc5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dc70cc552264bc09a28a6871ba4af91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f293f757b5cd4c12ac7119c22c500295": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c52d5d1529424339b2df1a68fdd5b2af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de09c40752174858b64c17ca4bd3c003": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e586aa23375402cbce02c34dae0ab1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dffdcc1a4eb34df1958ebefe0ee28d5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50dd1445cbe84e469e404074f1b6b7ee",
              "IPY_MODEL_27e9bb223c3f479e96ab755f92510ad9",
              "IPY_MODEL_4e26232981344f428e910f24281f0268"
            ],
            "layout": "IPY_MODEL_642c9cc09eea45be8c2c2eea338b24d3"
          }
        },
        "50dd1445cbe84e469e404074f1b6b7ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e80cee1e537140d1a9d6fe8a02fd77b4",
            "placeholder": "​",
            "style": "IPY_MODEL_f73c37722e4444a0b401e504d84cb878",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "27e9bb223c3f479e96ab755f92510ad9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f41150e876764474a593f09729b58c6b",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_558aeb09430c46ae9c1725629e820c0d",
            "value": 231508
          }
        },
        "4e26232981344f428e910f24281f0268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08fa87bbb3f543eaa6d1e0d55651f604",
            "placeholder": "​",
            "style": "IPY_MODEL_dca43062516843a2a09756b8b7776517",
            "value": " 232k/232k [00:00&lt;00:00, 1.78MB/s]"
          }
        },
        "642c9cc09eea45be8c2c2eea338b24d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e80cee1e537140d1a9d6fe8a02fd77b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f73c37722e4444a0b401e504d84cb878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f41150e876764474a593f09729b58c6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "558aeb09430c46ae9c1725629e820c0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "08fa87bbb3f543eaa6d1e0d55651f604": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dca43062516843a2a09756b8b7776517": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94b350404a7d4285bf4cf05069bae7df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b6c2ef7bb0f4de1acff3b8be4a91025",
              "IPY_MODEL_f76172fb2d9f49a39f2dd2853fb7034d",
              "IPY_MODEL_ba2539595e5e4939960563b81ce0e7a5"
            ],
            "layout": "IPY_MODEL_60196697153743a89ab45f4d54252204"
          }
        },
        "1b6c2ef7bb0f4de1acff3b8be4a91025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ab1acef3143422d89c61ffa55e8c576",
            "placeholder": "​",
            "style": "IPY_MODEL_09093a9af5134fad9022c5e24e360867",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "f76172fb2d9f49a39f2dd2853fb7034d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b0794194cac45668e5fc573e9dd4120",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b6752bcc7474e4e8a169d688b37a6f4",
            "value": 466062
          }
        },
        "ba2539595e5e4939960563b81ce0e7a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_697883f1f97d4a3b8e7fbbcde159e216",
            "placeholder": "​",
            "style": "IPY_MODEL_08088dc8b6d943d1a0372df028bb3724",
            "value": " 466k/466k [00:00&lt;00:00, 6.82MB/s]"
          }
        },
        "60196697153743a89ab45f4d54252204": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ab1acef3143422d89c61ffa55e8c576": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09093a9af5134fad9022c5e24e360867": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b0794194cac45668e5fc573e9dd4120": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b6752bcc7474e4e8a169d688b37a6f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "697883f1f97d4a3b8e7fbbcde159e216": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08088dc8b6d943d1a0372df028bb3724": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "688ef1a5cc49439c838a3ba5d32336b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97c70ede968e4ef9a7dc9b6769668ab1",
              "IPY_MODEL_5325c6be261f4177b37acb84d5844f60",
              "IPY_MODEL_70013b75bdc44fb1af10665b37acfc09"
            ],
            "layout": "IPY_MODEL_0cdc0837a449447ebd6a1b775623cf1c"
          }
        },
        "97c70ede968e4ef9a7dc9b6769668ab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22d66ed56377425cb5dfd12b8acb4305",
            "placeholder": "​",
            "style": "IPY_MODEL_d5e559c8ab0f44e781c8a96632f82609",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "5325c6be261f4177b37acb84d5844f60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ce7db5caa56420d91af8054a30e2223",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_937bd9e1e4ec41ec8eaedb72c9f4c016",
            "value": 570
          }
        },
        "70013b75bdc44fb1af10665b37acfc09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dde216e84504c69bf2fe22ccb562d26",
            "placeholder": "​",
            "style": "IPY_MODEL_592c1bc941b74285be87c3263a373ab6",
            "value": " 570/570 [00:00&lt;00:00, 6.77kB/s]"
          }
        },
        "0cdc0837a449447ebd6a1b775623cf1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22d66ed56377425cb5dfd12b8acb4305": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5e559c8ab0f44e781c8a96632f82609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ce7db5caa56420d91af8054a30e2223": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "937bd9e1e4ec41ec8eaedb72c9f4c016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6dde216e84504c69bf2fe22ccb562d26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "592c1bc941b74285be87c3263a373ab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "664a00cb559f45a4adf8348d837b8dbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95e33d7e36084f6cbfc96d8bc79fef63",
              "IPY_MODEL_1b9deba799fe40dbafc1a6588781dc7c",
              "IPY_MODEL_bd86013320314e96b218c9c18679337f"
            ],
            "layout": "IPY_MODEL_cc82690e353e4be8b4ef9c551351c624"
          }
        },
        "95e33d7e36084f6cbfc96d8bc79fef63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f398d4b899f64017bfd29e8c4b039868",
            "placeholder": "​",
            "style": "IPY_MODEL_d21dd63e74ef44d2812f6793d2512eb1",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "1b9deba799fe40dbafc1a6588781dc7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b92bd1ddb83471f94c33ad5ded2fb99",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35636e6040b84ee3ac7387769b1143c1",
            "value": 440449768
          }
        },
        "bd86013320314e96b218c9c18679337f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9610d68c06814bd3a461868e8d12dd9e",
            "placeholder": "​",
            "style": "IPY_MODEL_2d1b12a4e86e43c0a0b68bd8a1af2c51",
            "value": " 440M/440M [00:05&lt;00:00, 31.3MB/s]"
          }
        },
        "cc82690e353e4be8b4ef9c551351c624": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f398d4b899f64017bfd29e8c4b039868": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d21dd63e74ef44d2812f6793d2512eb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b92bd1ddb83471f94c33ad5ded2fb99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35636e6040b84ee3ac7387769b1143c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9610d68c06814bd3a461868e8d12dd9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d1b12a4e86e43c0a0b68bd8a1af2c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhish2702/BTP-Depression-Detection-/blob/main/btp_using_fasttext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M7qKxdrZnT5",
        "outputId": "2b242a7b-35ea-4b40-d779-4cd8f6df63a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_tweets_dataset=pd.read_csv(\"/content/clean_d_tweets.csv\")\n",
        "non_d_tweets_dataset=pd.read_csv(\"/content/clean_non_d_tweets.csv\")\n",
        "d_tweets_dataset.tweet = d_tweets_dataset.tweet.fillna('')\n",
        "non_d_tweets_dataset.tweet = non_d_tweets_dataset.tweet.fillna('')"
      ],
      "metadata": {
        "id": "OJK8TevKZuCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_tweets=d_tweets_dataset['tweet']"
      ],
      "metadata": {
        "id": "jIiLlB5cZ2J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_d_tweets=non_d_tweets_dataset['tweet']"
      ],
      "metadata": {
        "id": "G4odwZIBZ4QP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y1=np.ones((3082,1),dtype=\"int\")"
      ],
      "metadata": {
        "id": "Z4xFPGHdZ6sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y2=np.zeros((2505,1),dtype=\"int\")"
      ],
      "metadata": {
        "id": "zL7nDzNAZ8-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=np.concatenate((y1,y2),axis=0)"
      ],
      "metadata": {
        "id": "-j1Szd2kaAUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res=pd.DataFrame(y)"
      ],
      "metadata": {
        "id": "71EjJw9BaE_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt=pd.concat([d_tweets,non_d_tweets],axis=0)"
      ],
      "metadata": {
        "id": "-Xd8EF9_aHoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt.shape\n",
        "txt=txt.reset_index(drop=\"true\")"
      ],
      "metadata": {
        "id": "ZTUg-bEtaJ9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.concat([txt,res],axis=1,join=\"inner\")"
      ],
      "metadata": {
        "id": "jyxU_c9daQBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns=[\"text\",\"result\"]\n"
      ],
      "metadata": {
        "id": "GXnucdd2aSuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "RYMhNHCkaVe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = [word_tokenize(text.lower()) for text in df['text']]"
      ],
      "metadata": {
        "id": "CNSuS8NNaemX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "# Train FastText model\n",
        "fasttext_model = FastText(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n"
      ],
      "metadata": {
        "id": "PXl5De4SaqcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the FastText word vectors\n",
        "word_vectors = fasttext_model.wv\n"
      ],
      "metadata": {
        "id": "KAXqjWHcavMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to FastText word embeddings\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokenized_text)\n",
        "X_sequences = tokenizer.texts_to_sequences(tokenized_text)\n",
        "X_padded = pad_sequences(X_sequences, maxlen=100)\n",
        "\n",
        "# X_fasttext_padded = pad_sequences(X_fasttext, maxlen=100)\n",
        "# X_fasttext_padded\n",
        "# y = df['result'].values\n"
      ],
      "metadata": {
        "id": "S2WfmrR_azjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare target variable y (same as before)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "I4bDh90rbUr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_fasttext = []\n",
        "for sentence in tokenized_text:\n",
        "    embedding = []\n",
        "    for word in sentence:\n",
        "        if word in word_vectors:\n",
        "            embedding.append(word_vectors[word])\n",
        "    X_fasttext.append(embedding)\n",
        "\n",
        "# Find the maximum sequence length in X_fasttext\n",
        "max_sequence_length = max(len(embedding) for embedding in X_fasttext)\n",
        "\n",
        "# Pad sequences to the maximum length\n",
        "X_fasttext_padded = pad_sequences(X_fasttext, maxlen=100, dtype='float32', padding='post')\n",
        "X_fasttext_padded.shape\n",
        "y = df['result'].values"
      ],
      "metadata": {
        "id": "-LzqVWYJbbFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_fasttext_padded, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "beRS6NGCem6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fasttext with LSTM"
      ],
      "metadata": {
        "id": "LR0QzxSaDBaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from gensim.models import FastText\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Train your FastText model\n",
        "sentences = [text.split() for text in df['text']]\n",
        "fasttext_model = FastText(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "embedding_dim = 100  # Should match the vector_size in FastText model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, input_length=maxlen))\n",
        "model.add(LSTM(64))\n",
        "model.add((Dense(64,activation=\"relu\")))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(classification_report(y_val, y_pred,digits=6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3sFCIDMe1cY",
        "outputId": "ae2bff7d-2429-4ca9-8ca3-0484a383e3a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n",
            "Epoch 1/10\n",
            "140/140 [==============================] - 16s 90ms/step - loss: 0.4731 - accuracy: 0.7671 - val_loss: 0.3024 - val_accuracy: 0.8658\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 12s 86ms/step - loss: 0.2019 - accuracy: 0.9255 - val_loss: 0.2852 - val_accuracy: 0.8792\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 12s 86ms/step - loss: 0.1089 - accuracy: 0.9593 - val_loss: 0.3164 - val_accuracy: 0.8775\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 11s 79ms/step - loss: 0.0689 - accuracy: 0.9770 - val_loss: 0.3552 - val_accuracy: 0.8819\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 12s 87ms/step - loss: 0.0459 - accuracy: 0.9834 - val_loss: 0.4700 - val_accuracy: 0.8757\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 12s 86ms/step - loss: 0.0338 - accuracy: 0.9881 - val_loss: 0.5430 - val_accuracy: 0.8712\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 14s 100ms/step - loss: 0.0251 - accuracy: 0.9913 - val_loss: 0.4945 - val_accuracy: 0.8828\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 12s 88ms/step - loss: 0.0185 - accuracy: 0.9931 - val_loss: 0.6817 - val_accuracy: 0.8703\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 12s 87ms/step - loss: 0.0264 - accuracy: 0.9908 - val_loss: 0.5674 - val_accuracy: 0.8721\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 11s 79ms/step - loss: 0.0289 - accuracy: 0.9895 - val_loss: 0.6293 - val_accuracy: 0.8819\n",
            "35/35 [==============================] - 1s 17ms/step\n",
            "Validation Accuracy: 0.8819320214669052\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.883673  0.852362  0.867735       508\n",
            "           1   0.880573  0.906557  0.893376       610\n",
            "\n",
            "    accuracy                       0.881932      1118\n",
            "   macro avg   0.882123  0.879460  0.880556      1118\n",
            "weighted avg   0.881982  0.881932  0.881726      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Glove with LSTM"
      ],
      "metadata": {
        "id": "jq7SraQRDVQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Load the pre-trained GloVe embeddings\n",
        "glove_file = '/content/glove.6B.100d.txt'  # Change this to the actual path of your downloaded GloVe file\n",
        "embedding_dim = 100  # Should match the dimensionality of your pre-trained GloVe vectors\n",
        "\n",
        "# Create an embedding matrix\n",
        "embedding_matrix = {}\n",
        "with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_matrix[word] = coefs\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words and word in embedding_matrix:\n",
        "        embedding_matrix[i] = embedding_matrix[word]\n",
        "\n",
        "# Build the BiLSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(classification_report(y_val, y_pred,digits=6))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc1b927-dbfb-4d45-c294-1a5f4fcc96e7",
        "id": "P4eSeThmt8j9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-56-a5059f61375e>:41: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  if i < max_words and word in embedding_matrix:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "140/140 [==============================] - 12s 70ms/step - loss: 0.6887 - accuracy: 0.5505 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 7s 53ms/step - loss: 0.6879 - accuracy: 0.5531 - val_loss: 0.6922 - val_accuracy: 0.5456\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 12s 83ms/step - loss: 0.6881 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 10s 72ms/step - loss: 0.6878 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 7s 52ms/step - loss: 0.6881 - accuracy: 0.5531 - val_loss: 0.6891 - val_accuracy: 0.5456\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 9s 66ms/step - loss: 0.6878 - accuracy: 0.5531 - val_loss: 0.6893 - val_accuracy: 0.5456\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 8s 56ms/step - loss: 0.6877 - accuracy: 0.5531 - val_loss: 0.6892 - val_accuracy: 0.5456\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 8s 60ms/step - loss: 0.6878 - accuracy: 0.5531 - val_loss: 0.6892 - val_accuracy: 0.5456\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 10s 68ms/step - loss: 0.6876 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 7s 52ms/step - loss: 0.6877 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "35/35 [==============================] - 2s 27ms/step\n",
            "Validation Accuracy: 0.5456171735241503\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.000000  0.000000  0.000000       508\n",
            "           1   0.545617  1.000000  0.706019       610\n",
            "\n",
            "    accuracy                       0.545617      1118\n",
            "   macro avg   0.272809  0.500000  0.353009      1118\n",
            "weighted avg   0.297698  0.545617  0.385216      1118\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using TF-IDF with LSTM"
      ],
      "metadata": {
        "id": "QGo4-Hp0Hppb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "yF16UI6bkvFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['text']\n",
        "y = df['result']"
      ],
      "metadata": {
        "id": "8m9jEcaNHsmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "dwRS3F2OH0Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "qwkeauIAH6Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "6DUNdvEuIAo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_length = max(max(len(seq) for seq in X_train_seq), max(len(seq) for seq in X_test_seq))\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n"
      ],
      "metadata": {
        "id": "ecA9qansIDIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length))\n",
        "model.add(LSTM(128))\n",
        "model.add((Dense(64,activation=\"relu\")))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "yCq0YoWdIF4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "7a1IQLzIIIlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_split=0.1)\n"
      ],
      "metadata": {
        "id": "LXY87u3WIMUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90838410-bade-4110-cec4-8860f8c7b3ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "126/126 [==============================] - 19s 132ms/step - loss: 0.6891 - accuracy: 0.5485 - val_loss: 0.6920 - val_accuracy: 0.5280\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 19s 154ms/step - loss: 0.6884 - accuracy: 0.5602 - val_loss: 0.6926 - val_accuracy: 0.5280\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 17s 132ms/step - loss: 0.6188 - accuracy: 0.6658 - val_loss: 0.6617 - val_accuracy: 0.6063\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 16s 128ms/step - loss: 0.6003 - accuracy: 0.6412 - val_loss: 0.6453 - val_accuracy: 0.5772\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 16s 126ms/step - loss: 0.6399 - accuracy: 0.5592 - val_loss: 0.6631 - val_accuracy: 0.5280\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 16s 129ms/step - loss: 0.6361 - accuracy: 0.5472 - val_loss: 0.6605 - val_accuracy: 0.5391\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 16s 124ms/step - loss: 0.6327 - accuracy: 0.5629 - val_loss: 0.6594 - val_accuracy: 0.5414\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 16s 125ms/step - loss: 0.6298 - accuracy: 0.5549 - val_loss: 0.6565 - val_accuracy: 0.5503\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 16s 127ms/step - loss: 0.6284 - accuracy: 0.5617 - val_loss: 0.6527 - val_accuracy: 0.5280\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 16s 126ms/step - loss: 0.6268 - accuracy: 0.5664 - val_loss: 0.6541 - val_accuracy: 0.5548\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ab41b71e6b0>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred_probs = model.predict(X_test_pad)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(classification_report(y_val, y_pred,digits=6))"
      ],
      "metadata": {
        "id": "-8ZxOuHgIPcP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe9dda58-4bf4-4031-cc79-5fdbc2335c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35/35 [==============================] - 3s 54ms/step\n",
            "Validation Accuracy: 0.5813953488372093\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.520747  0.988189  0.682065       508\n",
            "           1   0.961039  0.242623  0.387435       610\n",
            "\n",
            "    accuracy                       0.581395      1118\n",
            "   macro avg   0.740893  0.615406  0.534750      1118\n",
            "weighted avg   0.760978  0.581395  0.521310      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using LSTM and word2vec"
      ],
      "metadata": {
        "id": "Vmu3c7EyrFqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "sentences = [text.split() for text in df['text']]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "embedding_dim = 100  # Should match the vector_size in Word2Vec model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(LSTM(64))\n",
        "model.add((Dense(64,activation=\"relu\")))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(classification_report(y_val, y_pred,digits=6))"
      ],
      "metadata": {
        "id": "kfFAbd1-IYNx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5372c11a-bf3e-43cd-d667-2e31fbdc7159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n",
            "Found 6236 unique tokens.\n",
            "Epoch 1/10\n",
            "140/140 [==============================] - 18s 95ms/step - loss: 0.4469 - accuracy: 0.7834 - val_loss: 0.2897 - val_accuracy: 0.8649\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 13s 90ms/step - loss: 0.1972 - accuracy: 0.9239 - val_loss: 0.2688 - val_accuracy: 0.8855\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 13s 90ms/step - loss: 0.1068 - accuracy: 0.9613 - val_loss: 0.3454 - val_accuracy: 0.8810\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 13s 91ms/step - loss: 0.0725 - accuracy: 0.9747 - val_loss: 0.3964 - val_accuracy: 0.8828\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 12s 89ms/step - loss: 0.0539 - accuracy: 0.9796 - val_loss: 0.4369 - val_accuracy: 0.8873\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 12s 84ms/step - loss: 0.0358 - accuracy: 0.9875 - val_loss: 0.5221 - val_accuracy: 0.8712\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 14s 100ms/step - loss: 0.0246 - accuracy: 0.9910 - val_loss: 0.6518 - val_accuracy: 0.8792\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 13s 92ms/step - loss: 0.0158 - accuracy: 0.9940 - val_loss: 0.6262 - val_accuracy: 0.8819\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 13s 91ms/step - loss: 0.0510 - accuracy: 0.9814 - val_loss: 0.5349 - val_accuracy: 0.8757\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 13s 92ms/step - loss: 0.0241 - accuracy: 0.9904 - val_loss: 0.6376 - val_accuracy: 0.8784\n",
            "35/35 [==============================] - 1s 18ms/step\n",
            "Validation Accuracy: 0.8783542039355993\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.899142  0.824803  0.860370       508\n",
            "           1   0.863497  0.922951  0.892235       610\n",
            "\n",
            "    accuracy                       0.878354      1118\n",
            "   macro avg   0.881319  0.873877  0.876302      1118\n",
            "weighted avg   0.879693  0.878354  0.877756      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using BiLSTM and word2vec"
      ],
      "metadata": {
        "id": "gIIw-4hmcRsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "9Es3bNLqca3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100"
      ],
      "metadata": {
        "id": "uvLTAC4qdxOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])"
      ],
      "metadata": {
        "id": "mluvnZned0jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhlyLWRDd4GD",
        "outputId": "4a78aadd-d3d4-4ee8-c6d1-20a28419b806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values"
      ],
      "metadata": {
        "id": "L56o9eb3d7En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5wM8KDQd-Ei",
        "outputId": "158c5000-a6b0-428a-82be-cad52bf93cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [text.split() for text in df['text']]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n"
      ],
      "metadata": {
        "id": "G-ysGtiLgYST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100  # Should match the vector_size in Word2Vec model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add((Dense(64,activation=\"relu\")))\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "DcmsjemKgb6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(classification_report(y_val, y_pred,digits=6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coXTClRkgg7m",
        "outputId": "47756dd3-0767-4dc5-fc76-31e0a3bb2359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "140/140 [==============================] - 25s 141ms/step - loss: 0.4511 - accuracy: 0.7821 - val_loss: 0.2841 - val_accuracy: 0.8712\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 19s 136ms/step - loss: 0.1887 - accuracy: 0.9250 - val_loss: 0.2940 - val_accuracy: 0.8784\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 21s 153ms/step - loss: 0.1161 - accuracy: 0.9568 - val_loss: 0.3411 - val_accuracy: 0.8936\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 22s 155ms/step - loss: 0.0665 - accuracy: 0.9770 - val_loss: 0.3665 - val_accuracy: 0.8918\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 22s 155ms/step - loss: 0.0528 - accuracy: 0.9819 - val_loss: 0.4972 - val_accuracy: 0.8631\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 19s 139ms/step - loss: 0.0406 - accuracy: 0.9850 - val_loss: 0.5104 - val_accuracy: 0.8739\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 20s 140ms/step - loss: 0.0306 - accuracy: 0.9877 - val_loss: 0.5530 - val_accuracy: 0.8828\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 21s 152ms/step - loss: 0.0458 - accuracy: 0.9828 - val_loss: 0.5521 - val_accuracy: 0.8792\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 20s 141ms/step - loss: 0.0156 - accuracy: 0.9942 - val_loss: 0.6324 - val_accuracy: 0.8792\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 20s 140ms/step - loss: 0.0114 - accuracy: 0.9957 - val_loss: 0.6529 - val_accuracy: 0.8828\n",
            "35/35 [==============================] - 2s 27ms/step\n",
            "Validation Accuracy: 0.8828264758497316\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.883910  0.854331  0.868869       508\n",
            "           1   0.881978  0.906557  0.894099       610\n",
            "\n",
            "    accuracy                       0.882826      1118\n",
            "   macro avg   0.882944  0.880444  0.881484      1118\n",
            "weighted avg   0.882856  0.882826  0.882635      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using BiLSTM and fasttext"
      ],
      "metadata": {
        "id": "8SgH-dj0olEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.models import FastText\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Train your FastText model\n",
        "sentences = [text.split() for text in df['text']]\n",
        "fasttext_model = FastText(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Build the BiLSTM model\n",
        "embedding_dim = 100  # Should match the vector_size in FastText model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, input_length=maxlen))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add((Dense(64,activation=\"relu\")))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(classification_report(y_val, y_pred,digits=6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOXci1wtgkvA",
        "outputId": "a75422f4-37af-43b5-92ba-fa5c91a94f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n",
            "Epoch 1/10\n",
            "140/140 [==============================] - 26s 147ms/step - loss: 0.4437 - accuracy: 0.7856 - val_loss: 0.2871 - val_accuracy: 0.8667\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 21s 148ms/step - loss: 0.1910 - accuracy: 0.9266 - val_loss: 0.2884 - val_accuracy: 0.8828\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 20s 141ms/step - loss: 0.1140 - accuracy: 0.9633 - val_loss: 0.3783 - val_accuracy: 0.8506\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 19s 138ms/step - loss: 0.0849 - accuracy: 0.9707 - val_loss: 0.3553 - val_accuracy: 0.8909\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 24s 170ms/step - loss: 0.0489 - accuracy: 0.9810 - val_loss: 0.4780 - val_accuracy: 0.8801\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 19s 139ms/step - loss: 0.0361 - accuracy: 0.9866 - val_loss: 0.5551 - val_accuracy: 0.8631\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 19s 139ms/step - loss: 0.0287 - accuracy: 0.9890 - val_loss: 0.5806 - val_accuracy: 0.8784\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 21s 150ms/step - loss: 0.0207 - accuracy: 0.9917 - val_loss: 0.8208 - val_accuracy: 0.8542\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 20s 140ms/step - loss: 0.0320 - accuracy: 0.9868 - val_loss: 0.5934 - val_accuracy: 0.8667\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 19s 138ms/step - loss: 0.0200 - accuracy: 0.9935 - val_loss: 0.6912 - val_accuracy: 0.8694\n",
            "35/35 [==============================] - 2s 28ms/step\n",
            "Validation Accuracy: 0.8694096601073346\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.886752  0.816929  0.850410       508\n",
            "           1   0.856923  0.913115  0.884127       610\n",
            "\n",
            "    accuracy                       0.869410      1118\n",
            "   macro avg   0.871838  0.865022  0.867268      1118\n",
            "weighted avg   0.870477  0.869410  0.868806      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dLLYSiDYon2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using BiLSTM and Glove"
      ],
      "metadata": {
        "id": "UWkAYjsuMWVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install glove-python"
      ],
      "metadata": {
        "id": "FJnD8MdFMbT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install glove_python"
      ],
      "metadata": {
        "id": "UyUaPZCuOgpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Load the pre-trained GloVe embeddings\n",
        "glove_file = '/content/glove.6B.100d.txt'  # Change this to the actual path of your downloaded GloVe file\n",
        "embedding_dim = 100  # Should match the dimensionality of your pre-trained GloVe vectors\n",
        "\n",
        "# Create an embedding matrix\n",
        "embedding_matrix = {}\n",
        "with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_matrix[word] = coefs\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words and word in embedding_matrix:\n",
        "        embedding_matrix[i] = embedding_matrix[word]\n",
        "\n",
        "# Build the BiLSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(classification_report(y_val, y_pred,digits=6))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ4Y8Ye8OpV5",
        "outputId": "191f6603-0f68-421e-c234-d5fff1c46c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-a3cdc3759390>:41: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  if i < max_words and word in embedding_matrix:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "140/140 [==============================] - 20s 117ms/step - loss: 0.6886 - accuracy: 0.5522 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 16s 111ms/step - loss: 0.6879 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 15s 111ms/step - loss: 0.6881 - accuracy: 0.5531 - val_loss: 0.6893 - val_accuracy: 0.5456\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 18s 132ms/step - loss: 0.6878 - accuracy: 0.5531 - val_loss: 0.6893 - val_accuracy: 0.5456\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 18s 126ms/step - loss: 0.6876 - accuracy: 0.5531 - val_loss: 0.6895 - val_accuracy: 0.5456\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 16s 113ms/step - loss: 0.6877 - accuracy: 0.5531 - val_loss: 0.6891 - val_accuracy: 0.5456\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 19s 133ms/step - loss: 0.6877 - accuracy: 0.5531 - val_loss: 0.6895 - val_accuracy: 0.5456\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 17s 122ms/step - loss: 0.6880 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 16s 113ms/step - loss: 0.6878 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 16s 114ms/step - loss: 0.6877 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "35/35 [==============================] - 2s 34ms/step\n",
            "Validation Accuracy: 0.5456171735241503\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.000000  0.000000  0.000000       508\n",
            "           1   0.545617  1.000000  0.706019       610\n",
            "\n",
            "    accuracy                       0.545617      1118\n",
            "   macro avg   0.272809  0.500000  0.353009      1118\n",
            "weighted avg   0.297698  0.545617  0.385216      1118\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(classification_report(y_val, y_pred,digits=6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R4yrzeCnlDn",
        "outputId": "2f279c6e-b936-4e7b-934e-b122481a4ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35/35 [==============================] - 2s 49ms/step\n",
            "Validation Accuracy: 0.5456171735241503\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.000000  0.000000  0.000000       508\n",
            "           1   0.545617  1.000000  0.706019       610\n",
            "\n",
            "    accuracy                       0.545617      1118\n",
            "   macro avg   0.272809  0.500000  0.353009      1118\n",
            "weighted avg   0.297698  0.545617  0.385216      1118\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz8YtDbDO8wV",
        "outputId": "da69135b-e97d-49b0-8833-5ab67429b8c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35/35 [==============================] - 6s 66ms/step\n",
            "Validation Accuracy: 0.5456171735241503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using BiLSTM and tf-idf"
      ],
      "metadata": {
        "id": "Q7K7EqrhRzg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=max_words)\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(train_data['text'])\n",
        "tfidf_val = tfidf_vectorizer.transform(val_data['text'])\n",
        "\n",
        "# Padding sequences\n",
        "x_train = pad_sequences(tfidf_train.toarray(), maxlen=maxlen)\n",
        "x_val = pad_sequences(tfidf_val.toarray(), maxlen=maxlen)\n",
        "\n",
        "y_train = train_data['result'].values\n",
        "y_val = val_data['result'].values\n",
        "\n",
        "# Build the BiLSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 128, input_length=maxlen))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHQIUuV3R2QE",
        "outputId": "e082a929-51e9-425c-b186-ed8ce88b1833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "140/140 [==============================] - 30s 166ms/step - loss: 0.6884 - accuracy: 0.5531 - val_loss: 0.6905 - val_accuracy: 0.5456\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 20s 146ms/step - loss: 0.6885 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 24s 174ms/step - loss: 0.6881 - accuracy: 0.5531 - val_loss: 0.6902 - val_accuracy: 0.5456\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 21s 148ms/step - loss: 0.6881 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 23s 167ms/step - loss: 0.6881 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 20s 146ms/step - loss: 0.6881 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 23s 163ms/step - loss: 0.6877 - accuracy: 0.5531 - val_loss: 0.6892 - val_accuracy: 0.5456\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 21s 151ms/step - loss: 0.6876 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 23s 163ms/step - loss: 0.6879 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 21s 150ms/step - loss: 0.6878 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ab425228be0>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(classification_report(y_val, y_pred,digits=6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwxEuro8R3lj",
        "outputId": "b1cd38ea-15fe-407f-efb6-fdd2784b2247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35/35 [==============================] - 2s 37ms/step\n",
            "Validation Accuracy: 0.5456171735241503\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.000000  0.000000  0.000000       508\n",
            "           1   0.545617  1.000000  0.706019       610\n",
            "\n",
            "    accuracy                       0.545617      1118\n",
            "   macro avg   0.272809  0.500000  0.353009      1118\n",
            "weighted avg   0.297698  0.545617  0.385216      1118\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using BERT and word2vec"
      ],
      "metadata": {
        "id": "BetflC3gZqfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWowRKT6TOzh",
        "outputId": "02272c31-08d5-4200-da7d-5fb0a00bca40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.19.1-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.1/311.1 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d5tknL-Zxsw",
        "outputId": "8aceed19-3fe0-47fc-976a-2e45a829df74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, GlobalAveragePooling1D, Concatenate\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Train your Word2Vec model\n",
        "sentences = [text.split() for text in df['text']]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Tokenize using BERT tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Convert sentences to BERT input format\n",
        "bert_input = bert_tokenizer(df['text'].tolist(), padding=True, truncation=True, return_tensors='np')\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Generate BERT embeddings\n",
        "with torch.no_grad():\n",
        "    bert_outputs = bert_model(**bert_input)\n",
        "\n",
        "# Extract embeddings from BERT output\n",
        "bert_embeddings = bert_outputs['last_hidden_state']\n",
        "\n",
        "# Average pooling over the sequence dimension\n",
        "bert_avg_pooling = np.mean(bert_embeddings, axis=1)\n",
        "\n",
        "# Combine Word2Vec and BERT embeddings\n",
        "combined_embeddings = np.concatenate([word2vec_model.wv[word] for word in df['text']], axis=0)\n",
        "combined_embeddings = np.concatenate([combined_embeddings, bert_avg_pooling], axis=1)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_dim=combined_embeddings.shape[1]))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(combined_embeddings, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433,
          "referenced_widgets": [
            "fdfb8c851a2c410497e6cf9b34b45a5e",
            "d68eac06b63d489ea47d1c4f868aea25",
            "ca02c6baeeb141879a8efb0d519a25c1",
            "2364cd238c4d4019814753190d176de1",
            "7554fdc971d54be5805d74ba29be2563",
            "dcc3b12fc3674882a7c16b01681acc5e",
            "7dc70cc552264bc09a28a6871ba4af91",
            "f293f757b5cd4c12ac7119c22c500295",
            "c52d5d1529424339b2df1a68fdd5b2af",
            "de09c40752174858b64c17ca4bd3c003",
            "2e586aa23375402cbce02c34dae0ab1b",
            "dffdcc1a4eb34df1958ebefe0ee28d5f",
            "50dd1445cbe84e469e404074f1b6b7ee",
            "27e9bb223c3f479e96ab755f92510ad9",
            "4e26232981344f428e910f24281f0268",
            "642c9cc09eea45be8c2c2eea338b24d3",
            "e80cee1e537140d1a9d6fe8a02fd77b4",
            "f73c37722e4444a0b401e504d84cb878",
            "f41150e876764474a593f09729b58c6b",
            "558aeb09430c46ae9c1725629e820c0d",
            "08fa87bbb3f543eaa6d1e0d55651f604",
            "dca43062516843a2a09756b8b7776517",
            "94b350404a7d4285bf4cf05069bae7df",
            "1b6c2ef7bb0f4de1acff3b8be4a91025",
            "f76172fb2d9f49a39f2dd2853fb7034d",
            "ba2539595e5e4939960563b81ce0e7a5",
            "60196697153743a89ab45f4d54252204",
            "0ab1acef3143422d89c61ffa55e8c576",
            "09093a9af5134fad9022c5e24e360867",
            "7b0794194cac45668e5fc573e9dd4120",
            "2b6752bcc7474e4e8a169d688b37a6f4",
            "697883f1f97d4a3b8e7fbbcde159e216",
            "08088dc8b6d943d1a0372df028bb3724",
            "688ef1a5cc49439c838a3ba5d32336b5",
            "97c70ede968e4ef9a7dc9b6769668ab1",
            "5325c6be261f4177b37acb84d5844f60",
            "70013b75bdc44fb1af10665b37acfc09",
            "0cdc0837a449447ebd6a1b775623cf1c",
            "22d66ed56377425cb5dfd12b8acb4305",
            "d5e559c8ab0f44e781c8a96632f82609",
            "2ce7db5caa56420d91af8054a30e2223",
            "937bd9e1e4ec41ec8eaedb72c9f4c016",
            "6dde216e84504c69bf2fe22ccb562d26",
            "592c1bc941b74285be87c3263a373ab6",
            "664a00cb559f45a4adf8348d837b8dbd",
            "95e33d7e36084f6cbfc96d8bc79fef63",
            "1b9deba799fe40dbafc1a6588781dc7c",
            "bd86013320314e96b218c9c18679337f",
            "cc82690e353e4be8b4ef9c551351c624",
            "f398d4b899f64017bfd29e8c4b039868",
            "d21dd63e74ef44d2812f6793d2512eb1",
            "6b92bd1ddb83471f94c33ad5ded2fb99",
            "35636e6040b84ee3ac7387769b1143c1",
            "9610d68c06814bd3a461868e8d12dd9e",
            "2d1b12a4e86e43c0a0b68bd8a1af2c51"
          ]
        },
        "id": "UDQTUu3oZ1nF",
        "outputId": "58c3d2af-76b5-424e-e35d-a2e2bbf69582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdfb8c851a2c410497e6cf9b34b45a5e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dffdcc1a4eb34df1958ebefe0ee28d5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94b350404a7d4285bf4cf05069bae7df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "688ef1a5cc49439c838a3ba5d32336b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "664a00cb559f45a4adf8348d837b8dbd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-e38667f9a187>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Generate BERT embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mbert_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbert_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Concatenate, Embedding, GlobalAveragePooling1D\n",
        "import torch\n",
        "import os\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Train your Word2Vec model\n",
        "sentences = [text.split() for text in df['text']]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Tokenize using BERT tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Convert sentences to BERT input format\n",
        "bert_input = bert_tokenizer(df['text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Generate BERT embeddings\n",
        "with torch.no_grad():\n",
        "    bert_outputs = bert_model(**bert_input)\n",
        "\n",
        "# Extract embeddings from BERT output\n",
        "bert_embeddings = bert_outputs['last_hidden_state']\n",
        "\n",
        "# Average pooling over the sequence dimension\n",
        "bert_avg_pooling = np.mean(bert_embeddings.numpy(), axis=1)\n",
        "\n",
        "# Combine Word2Vec and BERT embeddings\n",
        "combined_embeddings = np.concatenate([word2vec_model.wv[word] for word in df['text']], axis=0)\n",
        "combined_embeddings = np.concatenate([combined_embeddings, bert_avg_pooling], axis=1)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_dim=combined_embeddings.shape[1]))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(combined_embeddings, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "iIA5fe4fZ6Gp",
        "outputId": "6cd9e2c4-d143-42c6-c912-dfa3e87fc1e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ac780eec5b5a>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mresolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUClusterResolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'grpc://'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'COLAB_TPU_ADDR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_connect_to_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_tpu_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'COLAB_TPU_ADDR'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Concatenate, Embedding, GlobalAveragePooling1D\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Train your Word2Vec model\n",
        "sentences = [text.split() for text in df['text']]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Tokenize using BERT tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Convert sentences to BERT input format\n",
        "bert_input = bert_tokenizer(df['text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Check if running on Google Colab and TPU is available\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "    # Enable TPU acceleration\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    strategy = tf.distribute.TPUStrategy(resolver)\n",
        "else:\n",
        "    strategy = None\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "with strategy.scope() if strategy else nullcontext():\n",
        "    bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Generate BERT embeddings\n",
        "    with torch.no_grad():\n",
        "        bert_outputs = bert_model(**bert_input)\n",
        "\n",
        "    # Extract embeddings from BERT output\n",
        "    bert_embeddings = bert_outputs['last_hidden_state']\n",
        "\n",
        "# Average pooling over the sequence dimension\n",
        "bert_avg_pooling = np.mean(bert_embeddings.numpy(), axis=1)\n",
        "\n",
        "# Combine Word2Vec and BERT embeddings\n",
        "combined_embeddings = np.concatenate([word2vec_model.wv[word] for word in df['text']], axis=0)\n",
        "combined_embeddings = np.concatenate([combined_embeddings, bert_avg_pooling], axis=1)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_dim=combined_embeddings.shape[1]))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(combined_embeddings, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "7qHbRh6taMBk",
        "outputId": "0ca03bd5-3c01-4ba8-d664-7ceac3ca9d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a330eb61602a>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Load pre-trained BERT model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstrategy\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nullcontext' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Concatenate, Embedding, GlobalAveragePooling1D\n",
        "import torch\n",
        "import os\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# Define nullcontext for Python versions < 3.7\n",
        "@contextmanager\n",
        "def nullcontext(enter_result=None):\n",
        "    yield enter_result\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Train your Word2Vec model\n",
        "sentences = [text.split() for text in df['text']]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Tokenize using BERT tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Convert sentences to BERT input format\n",
        "bert_input = bert_tokenizer(df['text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Check if running on Google Colab and TPU is available\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "    # Enable TPU acceleration\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    strategy = tf.distribute.TPUStrategy(resolver)\n",
        "else:\n",
        "    strategy = nullcontext()\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "with strategy:\n",
        "    bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Generate BERT embeddings\n",
        "    with torch.no_grad():\n",
        "        bert_outputs = bert_model(**bert_input)\n",
        "\n",
        "    # Extract embeddings from BERT output\n",
        "    bert_embeddings = bert_outputs['last_hidden_state']\n",
        "\n",
        "# Average pooling over the sequence dimension\n",
        "bert_avg_pooling = np.mean(bert_embeddings.numpy(), axis=1)\n",
        "\n",
        "# Combine Word2Vec and BERT embeddings\n",
        "combined_embeddings = np.concatenate([word2vec_model.wv[word] for word in df['text']], axis=0)\n",
        "combined_embeddings = np.concatenate([combined_embeddings, bert_avg_pooling], axis=1)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_dim=combined_embeddings.shape[1]))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(combined_embeddings, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUsw027xd6pE",
        "outputId": "936627b4-59b6-491c-b2a7-af1986149eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KVDIp4XFePza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using GRU and Word2vec"
      ],
      "metadata": {
        "id": "XIIb4X2NeySe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import Word2Vec\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, GlobalAveragePooling1D, GRU\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Train your Word2Vec model\n",
        "sentences = [text.split() for text in df['text']]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Use a GRU layer for text representation\n",
        "embedding_dim = 100  # Adjust based on your Word2Vec model dimension\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(GRU(64))  # You can adjust the number of units based on your needs\n",
        "# model.add(Dense(64,activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(classification_report(y_val, y_pred,digits=6))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOSApYq_e37o",
        "outputId": "92e1ced5-b806-4c66-840a-ce6c510db838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n",
            "Epoch 1/10\n",
            "140/140 [==============================] - 17s 100ms/step - loss: 0.4731 - accuracy: 0.7530 - val_loss: 0.2930 - val_accuracy: 0.8703\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 11s 79ms/step - loss: 0.1910 - accuracy: 0.9279 - val_loss: 0.3143 - val_accuracy: 0.8855\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 12s 85ms/step - loss: 0.1157 - accuracy: 0.9582 - val_loss: 0.3097 - val_accuracy: 0.8730\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 12s 84ms/step - loss: 0.0650 - accuracy: 0.9783 - val_loss: 0.4092 - val_accuracy: 0.8837\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 12s 86ms/step - loss: 0.0450 - accuracy: 0.9834 - val_loss: 0.4642 - val_accuracy: 0.8855\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 11s 77ms/step - loss: 0.0374 - accuracy: 0.9875 - val_loss: 0.4783 - val_accuracy: 0.8891\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 11s 80ms/step - loss: 0.0236 - accuracy: 0.9904 - val_loss: 0.5268 - val_accuracy: 0.8828\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 12s 86ms/step - loss: 0.0199 - accuracy: 0.9942 - val_loss: 0.5750 - val_accuracy: 0.8775\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 12s 84ms/step - loss: 0.0227 - accuracy: 0.9919 - val_loss: 0.5635 - val_accuracy: 0.8658\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 13s 90ms/step - loss: 0.0176 - accuracy: 0.9933 - val_loss: 0.5916 - val_accuracy: 0.8801\n",
            "35/35 [==============================] - 1s 22ms/step\n",
            "Validation Accuracy: 0.8801431127012522\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.863813  0.874016  0.868885       508\n",
            "           1   0.894040  0.885246  0.889621       610\n",
            "\n",
            "    accuracy                       0.880143      1118\n",
            "   macro avg   0.878926  0.879631  0.879253      1118\n",
            "weighted avg   0.880305  0.880143  0.880199      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mr2uhPpIfC-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using BiGRU and word2vec"
      ],
      "metadata": {
        "id": "mu4Lt9SpgeqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import Word2Vec\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Bidirectional, GRU\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Train your Word2Vec model\n",
        "sentences = [text.split() for text in df['text']]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Use a Bidirectional GRU layer for text representation\n",
        "embedding_dim = 100  # Adjust based on your Word2Vec model dimension\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Bidirectional(GRU(64)))  # You can adjust the number of units based on your needs\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(classification_report(y_val, y_pred,digits=6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XNPg4Pnght7",
        "outputId": "f72e612d-7e6b-4022-8945-ffbfaa68a1b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n",
            "Epoch 1/10\n",
            "140/140 [==============================] - 29s 138ms/step - loss: 0.4736 - accuracy: 0.7440 - val_loss: 0.3156 - val_accuracy: 0.8560\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 19s 135ms/step - loss: 0.1965 - accuracy: 0.9237 - val_loss: 0.2827 - val_accuracy: 0.8819\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 21s 147ms/step - loss: 0.1069 - accuracy: 0.9638 - val_loss: 0.3350 - val_accuracy: 0.8739\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 19s 133ms/step - loss: 0.0590 - accuracy: 0.9808 - val_loss: 0.4061 - val_accuracy: 0.8837\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 19s 135ms/step - loss: 0.0364 - accuracy: 0.9881 - val_loss: 0.4216 - val_accuracy: 0.8855\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 21s 152ms/step - loss: 0.0465 - accuracy: 0.9834 - val_loss: 0.4576 - val_accuracy: 0.8766\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 21s 150ms/step - loss: 0.0285 - accuracy: 0.9906 - val_loss: 0.4786 - val_accuracy: 0.8757\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 19s 135ms/step - loss: 0.0168 - accuracy: 0.9946 - val_loss: 0.5832 - val_accuracy: 0.8792\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 19s 134ms/step - loss: 0.0127 - accuracy: 0.9957 - val_loss: 0.5946 - val_accuracy: 0.8792\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 21s 149ms/step - loss: 0.0117 - accuracy: 0.9953 - val_loss: 0.5980 - val_accuracy: 0.8748\n",
            "35/35 [==============================] - 1s 23ms/step\n",
            "Validation Accuracy: 0.8747763864042933\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.873984  0.846457  0.860000       508\n",
            "           1   0.875399  0.898361  0.886731       610\n",
            "\n",
            "    accuracy                       0.874776      1118\n",
            "   macro avg   0.874692  0.872409  0.873366      1118\n",
            "weighted avg   0.874756  0.874776  0.874585      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using BiGRU and fasttext"
      ],
      "metadata": {
        "id": "ul7_-oiChyKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wor35w0h3PJ",
        "outputId": "9807d014-ae41-4ab0-9db4-5f2303e5c661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.23.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199772 sha256=212eaad7fd5ed77ee8745b2fd69ec43fa3fa8bd3cec301b0c07d7174be93ecdf\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import fasttext\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Bidirectional, GRU\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Train your FastText model\n",
        "sentences = [text.split() for text in df['text']]\n",
        "fasttext_model = fasttext.train_unsupervised(sentences, model='skipgram', dim=100, epoch=10)\n",
        "\n",
        "# Use a Bidirectional GRU layer for text representation\n",
        "embedding_dim = 100  # Should match the FastText embedding dimension\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen, trainable=False))\n",
        "model.add(Bidirectional(GRU(64)))  # You can adjust the number of units based on your needs\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Set the embedding matrix with FastText embeddings\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        try:\n",
        "            embedding_matrix[i] = fasttext_model[word]\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "EK7l_RVNgpXl",
        "outputId": "86176e11-a8e0-406d-a5a4-4f68642e1799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1591be0352c6>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Train your FastText model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mfasttext_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_unsupervised\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'skipgram'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Use a Bidirectional GRU layer for text representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fasttext/FastText.py\u001b[0m in \u001b[0;36mtrain_unsupervised\u001b[0;34m(*kargs, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m     args, manually_set_args = read_args(kargs, kwargs, arg_names,\n\u001b[1;32m    556\u001b[0m                                         unsupervised_default)\n\u001b[0;32m--> 557\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanually_set_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m     \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fasttext/FastText.py\u001b[0m in \u001b[0;36m_build_args\u001b[0;34m(args, manually_set_args)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmanually_set_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetManual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: (): incompatible function arguments. The following argument types are supported:\n    1. (self: fasttext_pybind.args, arg0: str) -> None\n\nInvoked with: <fasttext_pybind.args object at 0x79ed511174f0>, [['the', 'real', 'reason', 'why', 'you', 'be', 'sad', 'you', 'be', 'attach', 'to', 'people', 'who', 'have', 'be', 'distant', 'with', 'you', 'you', 'be', 'pay', 'attention', 'to', 'people', 'who', 'ignore', 'you', 'you', 'make', 'time', 'for', 'people', 'who', 'be', 'too', 'busy', 'for', 'you', 'you', 'be', 'too', 'care', 'to', 'people', 'who', 'be', 'care', 'less', 'when', 'it', 'come', 'to', 'you', 'let', 'those', 'people', 'go'], ['my', 'biggest', 'problem', 'be', 'overthinking', 'everything'], ['the', 'worst', 'sadness', 'be', 'the', 'sadness', 'you', 'have', 'teach', 'yourself', 'to', 'hide'], ['i', 'cannot', 'make', 'you', 'understand', 'i', 'cannot', 'make', 'anyone', 'understand', 'what', 'be', 'happen', 'inside', 'me', 'i', 'cannot', 'even', 'explain', 'it', 'to', 'myself'], ['i', 'do', 'not', 'think', 'anyone', 'really', 'understand', 'how', 'tire', 'it', 'be', 'to', 'act', 'okay', 'and', 'always', 'be', 'the', 'strong', 'one', 'when', 'in', 'reality', 'you', 'be', 'close', 'to', 'the', 'edge'], ['the', 'worst', 'feel', 'be', 'when', 'something', 'be', 'kill', 'you', 'inside', 'and', 'you', 'have', 'to', 'act', 'like', 'you', 'do', 'not', 'care'], ['when', 'i', 'be', 'hurt', 'i', 'shut', 'down', 'i', 'turn', 'into', 'a', 'total', 'bitch', 'i', 'shut', 'off', 'my', 'emotions', 'i', 'act', 'differently', 'towards', 'everything', 'and', 'everyone', 'and', 'i', 'hate', 'it'], ['overthinking', 'ruin', 'you'..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import FastText\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Bidirectional, GRU\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Train your FastText model\n",
        "sentences = [text.split() for text in df['text']]\n",
        "fasttext_model = FastText(sentences, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "\n",
        "# Use a Bidirectional GRU layer for text representation\n",
        "embedding_dim = 100  # Should match the FastText embedding dimension\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen, trainable=False))\n",
        "model.add(Bidirectional(GRU(64)))  # You can adjust the number of units based on your needs\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Set the embedding matrix with FastText embeddings\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        try:\n",
        "            embedding_matrix[i] = fasttext_model.wv[word]\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(classification_report(y_val, y_pred,digits=6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBXBoX_Sh0eZ",
        "outputId": "5e86a5fc-ed5d-4c91-c697-ffb71946eeec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n",
            "Epoch 1/10\n",
            "140/140 [==============================] - 20s 114ms/step - loss: 0.6031 - accuracy: 0.6657 - val_loss: 0.5506 - val_accuracy: 0.7174\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 15s 106ms/step - loss: 0.5120 - accuracy: 0.7480 - val_loss: 0.5065 - val_accuracy: 0.7513\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 15s 108ms/step - loss: 0.4998 - accuracy: 0.7590 - val_loss: 0.4694 - val_accuracy: 0.7907\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 19s 136ms/step - loss: 0.4880 - accuracy: 0.7608 - val_loss: 0.4722 - val_accuracy: 0.7961\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 16s 112ms/step - loss: 0.4734 - accuracy: 0.7713 - val_loss: 0.4619 - val_accuracy: 0.7889\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 16s 115ms/step - loss: 0.4629 - accuracy: 0.7753 - val_loss: 0.4408 - val_accuracy: 0.8086\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 15s 110ms/step - loss: 0.4534 - accuracy: 0.7850 - val_loss: 0.4328 - val_accuracy: 0.8068\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 15s 108ms/step - loss: 0.4462 - accuracy: 0.7872 - val_loss: 0.4306 - val_accuracy: 0.8131\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 15s 108ms/step - loss: 0.4418 - accuracy: 0.7926 - val_loss: 0.4482 - val_accuracy: 0.7791\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 15s 109ms/step - loss: 0.4503 - accuracy: 0.7899 - val_loss: 0.4246 - val_accuracy: 0.8014\n",
            "35/35 [==============================] - 2s 40ms/step\n",
            "Validation Accuracy: 0.8014311270125224\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.761905  0.818898  0.789374       508\n",
            "           1   0.839161  0.786885  0.812183       610\n",
            "\n",
            "    accuracy                       0.801431      1118\n",
            "   macro avg   0.800533  0.802891  0.800778      1118\n",
            "weighted avg   0.804057  0.801431  0.801819      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BYBr1XlHir_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using BiGRU and Glove"
      ],
      "metadata": {
        "id": "NIRim4Chj_ZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Bidirectional, GRU\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Download GloVe embeddings (you can choose a different dimension, e.g., 50, 100, 200, 300)\n",
        "glove_file = 'glove.6B.100d.txt'\n",
        "glove_w2v_file = 'glove.6B.100d.w2v.txt'\n",
        "\n",
        "# Convert GloVe format to Word2Vec format\n",
        "glove2word2vec(glove_file, glove_w2v_file)\n",
        "\n",
        "# Load the GloVe Word2Vec model\n",
        "glove_model = KeyedVectors.load_word2vec_format(glove_w2v_file, binary=False)\n",
        "\n",
        "# Use a Bidirectional GRU layer for text representation\n",
        "embedding_dim = 100  # Should match the GloVe embedding dimension (in this case, 100)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen, trainable=False))\n",
        "model.add(Bidirectional(GRU(64)))  # You can adjust the number of units based on your needs\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Set the embedding matrix with GloVe embeddings\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        try:\n",
        "            embedding_matrix[i] = glove_model[word]\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(classification_report(y_val, y_pred,digits=6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjnArjsykCBW",
        "outputId": "6477b699-32c7-4b98-9665-dd55c5eb8185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-9f34bc848794>:35: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
            "  glove2word2vec(glove_file, glove_w2v_file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "140/140 [==============================] - 55s 114ms/step - loss: 0.4779 - accuracy: 0.7659 - val_loss: 0.3924 - val_accuracy: 0.8184\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 15s 110ms/step - loss: 0.3517 - accuracy: 0.8416 - val_loss: 0.3110 - val_accuracy: 0.8640\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 15s 109ms/step - loss: 0.3248 - accuracy: 0.8554 - val_loss: 0.3034 - val_accuracy: 0.8676\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 15s 110ms/step - loss: 0.2977 - accuracy: 0.8747 - val_loss: 0.2917 - val_accuracy: 0.8694\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 15s 110ms/step - loss: 0.2896 - accuracy: 0.8785 - val_loss: 0.3098 - val_accuracy: 0.8587\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 15s 111ms/step - loss: 0.2631 - accuracy: 0.8881 - val_loss: 0.3185 - val_accuracy: 0.8649\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 15s 110ms/step - loss: 0.2503 - accuracy: 0.8924 - val_loss: 0.2769 - val_accuracy: 0.8801\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 15s 108ms/step - loss: 0.2342 - accuracy: 0.8998 - val_loss: 0.2961 - val_accuracy: 0.8703\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 17s 123ms/step - loss: 0.2123 - accuracy: 0.9098 - val_loss: 0.2856 - val_accuracy: 0.8801\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 15s 111ms/step - loss: 0.1901 - accuracy: 0.9226 - val_loss: 0.2761 - val_accuracy: 0.8819\n",
            "35/35 [==============================] - 1s 23ms/step\n",
            "Validation Accuracy: 0.8819320214669052\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.894958  0.838583  0.865854       508\n",
            "           1   0.872274  0.918033  0.894569       610\n",
            "\n",
            "    accuracy                       0.881932      1118\n",
            "   macro avg   0.883616  0.878308  0.880211      1118\n",
            "weighted avg   0.882581  0.881932  0.881521      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen, trainable=False))\n",
        "model.add(Bidirectional(GRU(64)))  # You can adjust the number of units based on your needs\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Set the embedding matrix with GloVe embeddings\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        try:\n",
        "            embedding_matrix[i] = glove_model[word]\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdt0ZC97kDTS",
        "outputId": "7c66ebde-a469-4234-c379-2a9c4224e6df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "140/140 [==============================] - 36s 187ms/step - loss: 0.4663 - accuracy: 0.7722 - val_loss: 0.3473 - val_accuracy: 0.8399\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 18s 128ms/step - loss: 0.3295 - accuracy: 0.8599 - val_loss: 0.3019 - val_accuracy: 0.8640\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 22s 157ms/step - loss: 0.2990 - accuracy: 0.8725 - val_loss: 0.2966 - val_accuracy: 0.8757\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 22s 154ms/step - loss: 0.2848 - accuracy: 0.8789 - val_loss: 0.2807 - val_accuracy: 0.8846\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 29s 207ms/step - loss: 0.2590 - accuracy: 0.8908 - val_loss: 0.3446 - val_accuracy: 0.8587\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 25s 174ms/step - loss: 0.2378 - accuracy: 0.8991 - val_loss: 0.2778 - val_accuracy: 0.8891\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 18s 130ms/step - loss: 0.2188 - accuracy: 0.9103 - val_loss: 0.3033 - val_accuracy: 0.8721\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 18s 128ms/step - loss: 0.1967 - accuracy: 0.9215 - val_loss: 0.2715 - val_accuracy: 0.8936\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 18s 129ms/step - loss: 0.1728 - accuracy: 0.9340 - val_loss: 0.3291 - val_accuracy: 0.8792\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 18s 127ms/step - loss: 0.1522 - accuracy: 0.9380 - val_loss: 0.3005 - val_accuracy: 0.8837\n",
            "35/35 [==============================] - 3s 45ms/step\n",
            "Validation Accuracy: 0.8837209302325582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using BiGRU and tf-idf"
      ],
      "metadata": {
        "id": "j21mVNhEmEf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Bidirectional, GRU\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = TfidfVectorizer(max_features=max_words)\n",
        "tfidf_matrix = tokenizer.fit_transform(df['text']).toarray()\n",
        "\n",
        "# Use a Bidirectional GRU layer for text representation\n",
        "embedding_dim = tfidf_matrix.shape[1]  # Use the number of features from TF-IDF\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_dim=embedding_dim))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(tfidf_matrix, df['result'].values, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NaXs8R6lf1i",
        "outputId": "0ee9529c-be10-4df0-8ae0-d23401f1d4e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "140/140 [==============================] - 6s 28ms/step - loss: 0.5767 - accuracy: 0.7389 - val_loss: 0.4321 - val_accuracy: 0.8524\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 3s 22ms/step - loss: 0.3188 - accuracy: 0.9112 - val_loss: 0.3109 - val_accuracy: 0.8649\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 2s 16ms/step - loss: 0.2005 - accuracy: 0.9427 - val_loss: 0.2796 - val_accuracy: 0.8685\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 2s 14ms/step - loss: 0.1423 - accuracy: 0.9604 - val_loss: 0.2699 - val_accuracy: 0.8748\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 2s 18ms/step - loss: 0.1067 - accuracy: 0.9693 - val_loss: 0.2807 - val_accuracy: 0.8685\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 0.0833 - accuracy: 0.9781 - val_loss: 0.2894 - val_accuracy: 0.8757\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 3s 23ms/step - loss: 0.0670 - accuracy: 0.9828 - val_loss: 0.2911 - val_accuracy: 0.8784\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 0.0548 - accuracy: 0.9866 - val_loss: 0.3073 - val_accuracy: 0.8801\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 3s 18ms/step - loss: 0.0458 - accuracy: 0.9888 - val_loss: 0.3133 - val_accuracy: 0.8792\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 2s 14ms/step - loss: 0.0391 - accuracy: 0.9908 - val_loss: 0.3289 - val_accuracy: 0.8792\n",
            "35/35 [==============================] - 0s 3ms/step\n",
            "Validation Accuracy: 0.8792486583184258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "\n",
        "tokenizer = TfidfVectorizer(max_features=max_words)\n",
        "tfidf_matrix = tokenizer.fit_transform(df['text']).toarray()\n",
        "\n",
        "# Use a Dense neural network for text representation\n",
        "embedding_dim = tfidf_matrix.shape[1]  # Use the number of features from TF-IDF\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_dim=embedding_dim))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(tfidf_matrix, df['result'].values, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Other metrics\n",
        "print(classification_report(y_val, y_pred,digits=6))"
      ],
      "metadata": {
        "id": "wYsr1zfNm0Po",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af3cf2ea-9458-46ab-cc48-b815f540cd42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "140/140 [==============================] - 3s 17ms/step - loss: 0.5886 - accuracy: 0.7843 - val_loss: 0.4429 - val_accuracy: 0.8551\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 2s 16ms/step - loss: 0.3255 - accuracy: 0.9114 - val_loss: 0.3152 - val_accuracy: 0.8631\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 2s 12ms/step - loss: 0.2033 - accuracy: 0.9432 - val_loss: 0.2804 - val_accuracy: 0.8640\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 2s 11ms/step - loss: 0.1431 - accuracy: 0.9602 - val_loss: 0.2700 - val_accuracy: 0.8676\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 1s 7ms/step - loss: 0.1070 - accuracy: 0.9689 - val_loss: 0.2703 - val_accuracy: 0.8757\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 1s 7ms/step - loss: 0.0839 - accuracy: 0.9763 - val_loss: 0.2884 - val_accuracy: 0.8694\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 1s 7ms/step - loss: 0.0673 - accuracy: 0.9828 - val_loss: 0.2989 - val_accuracy: 0.8739\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 1s 7ms/step - loss: 0.0550 - accuracy: 0.9857 - val_loss: 0.3008 - val_accuracy: 0.8801\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 1s 7ms/step - loss: 0.0459 - accuracy: 0.9893 - val_loss: 0.3162 - val_accuracy: 0.8775\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 1s 7ms/step - loss: 0.0390 - accuracy: 0.9913 - val_loss: 0.3304 - val_accuracy: 0.8792\n",
            "35/35 [==============================] - 0s 2ms/step\n",
            "Accuracy: 0.8792486583184258\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.902808  0.822835  0.860968       508\n",
            "           1   0.862595  0.926230  0.893281       610\n",
            "\n",
            "    accuracy                       0.879249      1118\n",
            "   macro avg   0.882702  0.874532  0.877124      1118\n",
            "weighted avg   0.880867  0.879249  0.878598      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jQSoLi6vaHAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using GRU with fasttext"
      ],
      "metadata": {
        "id": "XaU4dvk2yjbS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YACe0FqRymxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import FastText\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Bidirectional, GRU\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Train your FastText model\n",
        "sentences = [text.split() for text in df['text']]\n",
        "fasttext_model = FastText(sentences, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "\n",
        "# Use a Bidirectional GRU layer for text representation\n",
        "embedding_dim = 100  # Should match the FastText embedding dimension\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen, trainable=False))\n",
        "model.add(GRU(64))  # You can adjust the number of units based on your needs\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Set the embedding matrix with FastText embeddings\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        try:\n",
        "            embedding_matrix[i] = fasttext_model.wv[word]\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(classification_report(y_val, y_pred,digits=6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba64f9e1-ea6d-4dc6-b44e-d00f035041d7",
        "id": "QJIyFPLwyxBj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n",
            "Epoch 1/10\n",
            "140/140 [==============================] - 14s 65ms/step - loss: 0.5743 - accuracy: 0.6861 - val_loss: 0.5051 - val_accuracy: 0.7549\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 9s 68ms/step - loss: 0.5053 - accuracy: 0.7552 - val_loss: 0.4808 - val_accuracy: 0.7791\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 8s 55ms/step - loss: 0.4870 - accuracy: 0.7621 - val_loss: 0.4703 - val_accuracy: 0.7907\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 9s 66ms/step - loss: 0.4835 - accuracy: 0.7648 - val_loss: 0.4671 - val_accuracy: 0.7952\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 9s 61ms/step - loss: 0.4740 - accuracy: 0.7686 - val_loss: 0.4806 - val_accuracy: 0.7710\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 8s 59ms/step - loss: 0.4699 - accuracy: 0.7749 - val_loss: 0.4533 - val_accuracy: 0.7979\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 9s 67ms/step - loss: 0.4652 - accuracy: 0.7729 - val_loss: 0.4510 - val_accuracy: 0.7943\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 10s 74ms/step - loss: 0.4525 - accuracy: 0.7881 - val_loss: 0.4638 - val_accuracy: 0.7755\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 9s 62ms/step - loss: 0.4551 - accuracy: 0.7818 - val_loss: 0.4629 - val_accuracy: 0.7862\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 9s 67ms/step - loss: 0.4543 - accuracy: 0.7859 - val_loss: 0.4425 - val_accuracy: 0.8050\n",
            "35/35 [==============================] - 1s 15ms/step\n",
            "Validation Accuracy: 0.8050089445438283\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.845238  0.698819  0.765086       508\n",
            "           1   0.780802  0.893443  0.833333       610\n",
            "\n",
            "    accuracy                       0.805009      1118\n",
            "   macro avg   0.813020  0.796131  0.799210      1118\n",
            "weighted avg   0.810081  0.805009  0.802323      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xe-cwg39yxRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using GRU with Glove"
      ],
      "metadata": {
        "id": "_aHTaZ0jz-36"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a-eEmfZt0BUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Load the pre-trained GloVe embeddings\n",
        "glove_file = '/content/glove.6B.100d.txt'  # Change this to the actual path of your downloaded GloVe file\n",
        "embedding_dim = 100  # Should match the dimensionality of your pre-trained GloVe vectors\n",
        "\n",
        "# Create an embedding matrix\n",
        "embedding_matrix = {}\n",
        "with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_matrix[word] = coefs\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words and word in embedding_matrix:\n",
        "        embedding_matrix[i] = embedding_matrix[word]\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n",
        "model.add(GRU(64))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(classification_report(y_val, y_pred,digits=6))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65111de8-3004-453e-b6d8-8e9cd9bb552e",
        "id": "Us9PjFfq0BkT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-72-1a4869993b54>:41: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  if i < max_words and word in embedding_matrix:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "140/140 [==============================] - 12s 71ms/step - loss: 0.6884 - accuracy: 0.5522 - val_loss: 0.6898 - val_accuracy: 0.5456\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 8s 58ms/step - loss: 0.6878 - accuracy: 0.5531 - val_loss: 0.6893 - val_accuracy: 0.5456\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 9s 62ms/step - loss: 0.6881 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 9s 67ms/step - loss: 0.6883 - accuracy: 0.5531 - val_loss: 0.6898 - val_accuracy: 0.5456\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 7s 53ms/step - loss: 0.6877 - accuracy: 0.5531 - val_loss: 0.6891 - val_accuracy: 0.5456\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 9s 66ms/step - loss: 0.6879 - accuracy: 0.5531 - val_loss: 0.6891 - val_accuracy: 0.5456\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 7s 53ms/step - loss: 0.6877 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 9s 66ms/step - loss: 0.6880 - accuracy: 0.5531 - val_loss: 0.6896 - val_accuracy: 0.5456\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 12s 86ms/step - loss: 0.6877 - accuracy: 0.5531 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 8s 55ms/step - loss: 0.6879 - accuracy: 0.5531 - val_loss: 0.6892 - val_accuracy: 0.5456\n",
            "35/35 [==============================] - 1s 23ms/step\n",
            "Validation Accuracy: 0.5456171735241503\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.000000  0.000000  0.000000       508\n",
            "           1   0.545617  1.000000  0.706019       610\n",
            "\n",
            "    accuracy                       0.545617      1118\n",
            "   macro avg   0.272809  0.500000  0.353009      1118\n",
            "weighted avg   0.297698  0.545617  0.385216      1118\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using GRU with tf-idf"
      ],
      "metadata": {
        "id": "wc7o3-vR0nMo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c98GLgxE0FQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Bidirectional, GRU\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text using TF-IDF\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=max_words, stop_words=ENGLISH_STOP_WORDS)\n",
        "tfidf_matrix = vectorizer.fit_transform(df['text']).toarray()\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Use a Bidirectional GRU layer for text representation\n",
        "embedding_dim = 100  # You can adjust the dimension based on your needs\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen, trainable=False))\n",
        "model.add(GRU(64, return_sequences=True))  # You can adjust the number of units based on your needs\n",
        "model.add(GRU(32))  # You can adjust the number of units based on your needs\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Set the embedding matrix with TF-IDF embeddings\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        try:\n",
        "            embedding_matrix[i] = tfidf_matrix[:, word_index[word] - 1]\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(classification_report(y_val, y_pred,digits=6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "d3605054-dc70-42a0-9ff5-b9a4bc038c92",
        "id": "qHQYd-Vs0wm2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidParameterError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-4330addf1cd1>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mENGLISH_STOP_WORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtfidf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Tokenize the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2131\u001b[0m             \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m         )\n\u001b[0;32m-> 2133\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1367\u001b[0m             )\n\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_ngram_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0maccepted\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \"\"\"\n\u001b[0;32m--> 600\u001b[0;31m         validate_parameter_constraints(\n\u001b[0m\u001b[1;32m    601\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 )\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             raise InvalidParameterError(\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0;34mf\"The {param_name!r} parameter of {caller_name} must be\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;34mf\" {constraints_str}. Got {param_val!r} instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'every', 'except', 'beforehand', 'until', 'seems', 'out', 'though', 'un', 'we', 'will', 'besides', 'full', 'sixty', 'behind', 'herself', 'others', 'yet', 'thereby', 'per', 'thereupon', 'sometimes', 'etc', 'therein', 'nowhere', 'amoungst', 'again', 'much', 'now', 'thus', 'mostly', 'sometime', 'anywhere', 'under', 'hundred', 'other', 'call', 'beside', 'get', 'hence', 'yours', 'would', 'somewhere', 'towards', 'even', 'fire', 'he', 'must', 'cry', 'further', 'up', 'than', 'mine', 'forty', 'thru', 'found', 'hereby', 'yourselves', 'because', 'next', 'whereby', 'con', 'could', 'front', 'their', 'wherever', 'ie', 'two', 'everyone', 'his', 'seem', 'yourself', 'among', 'top', 'whereupon', 'always', 'please', 'himself', 'the', 'while', 'whoever', 'were', 'us', 'into', 'too', 'meanwhile', 'down', 'part', 'empty', 'describe', 'noone', 'from', 'them', 'have', 'such', 'over', 'on', 'sincere', 'neither', 'elsewhere', 'can', 'our', 'made', 'thence', 'former', 'show', 'inc', 'amongst', 'after', 'hereafter', 'has', 'wherein', 'been', 'of', 'to', 'by', 'rather', 'but', 'seeming', 'whole', 'system', 'had', 'four', 'either', 'becoming', 'namely', 'that', 'is', 'here', 'against', 'are', 'anyhow', 'ever', 'without', 'three', 'there', 'least', 'although', 'my', 'about', 'less', 'being', 'de', 'any', 'may', 'whose', 'several', 'through', 'name..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Bidirectional, GRU\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text using TF-IDF\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=max_words, stop_words=ENGLISH_STOP_WORDS)\n",
        "tfidf_matrix = vectorizer.fit_transform(df['text']).toarray()\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Use a Bidirectional GRU layer for text representation\n",
        "embedding_dim = 100  # You can adjust the dimension based on your needs\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen))\n",
        "model.add(Bidirectional(GRU(64, return_sequences=True)))\n",
        "model.add(Bidirectional(GRU(32)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Set the embedding matrix with TF-IDF weights\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        try:\n",
        "            embedding_matrix[i] = tfidf_matrix[:, word_index[word] - 1]\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "d4jQexUb1BYy",
        "outputId": "1ab7c382-ab67-46c1-8d35-e1c4fa36a6c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidParameterError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-2c8b9127129d>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mENGLISH_STOP_WORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtfidf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Tokenize the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2131\u001b[0m             \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m         )\n\u001b[0;32m-> 2133\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1367\u001b[0m             )\n\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_ngram_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0maccepted\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \"\"\"\n\u001b[0;32m--> 600\u001b[0;31m         validate_parameter_constraints(\n\u001b[0m\u001b[1;32m    601\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 )\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             raise InvalidParameterError(\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0;34mf\"The {param_name!r} parameter of {caller_name} must be\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;34mf\" {constraints_str}. Got {param_val!r} instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'every', 'except', 'beforehand', 'until', 'seems', 'out', 'though', 'un', 'we', 'will', 'besides', 'full', 'sixty', 'behind', 'herself', 'others', 'yet', 'thereby', 'per', 'thereupon', 'sometimes', 'etc', 'therein', 'nowhere', 'amoungst', 'again', 'much', 'now', 'thus', 'mostly', 'sometime', 'anywhere', 'under', 'hundred', 'other', 'call', 'beside', 'get', 'hence', 'yours', 'would', 'somewhere', 'towards', 'even', 'fire', 'he', 'must', 'cry', 'further', 'up', 'than', 'mine', 'forty', 'thru', 'found', 'hereby', 'yourselves', 'because', 'next', 'whereby', 'con', 'could', 'front', 'their', 'wherever', 'ie', 'two', 'everyone', 'his', 'seem', 'yourself', 'among', 'top', 'whereupon', 'always', 'please', 'himself', 'the', 'while', 'whoever', 'were', 'us', 'into', 'too', 'meanwhile', 'down', 'part', 'empty', 'describe', 'noone', 'from', 'them', 'have', 'such', 'over', 'on', 'sincere', 'neither', 'elsewhere', 'can', 'our', 'made', 'thence', 'former', 'show', 'inc', 'amongst', 'after', 'hereafter', 'has', 'wherein', 'been', 'of', 'to', 'by', 'rather', 'but', 'seeming', 'whole', 'system', 'had', 'four', 'either', 'becoming', 'namely', 'that', 'is', 'here', 'against', 'are', 'anyhow', 'ever', 'without', 'three', 'there', 'least', 'although', 'my', 'about', 'less', 'being', 'de', 'any', 'may', 'whose', 'several', 'through', 'name..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Bidirectional, GRU\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text using TF-IDF\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=max_words, stop_words=ENGLISH_STOP_WORDS)\n",
        "tfidf_matrix = vectorizer.fit_transform(df['text']).toarray()\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Use a Bidirectional GRU layer for text representation\n",
        "embedding_dim = tfidf_matrix.shape[1]  # Adjust based on the TF-IDF matrix dimension\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen, weights=[tfidf_matrix], trainable=False))\n",
        "model.add(Bidirectional(GRU(64, return_sequences=True)))\n",
        "model.add(Bidirectional(GRU(32)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs = model.predict(x_val)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "PHObuYue175W",
        "outputId": "33366ff8-9a4f-4d15-94d4-9f53bb05470d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidParameterError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-ce15bad5d3c4>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mENGLISH_STOP_WORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtfidf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Tokenize the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2131\u001b[0m             \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m         )\n\u001b[0;32m-> 2133\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1367\u001b[0m             )\n\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_ngram_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0maccepted\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \"\"\"\n\u001b[0;32m--> 600\u001b[0;31m         validate_parameter_constraints(\n\u001b[0m\u001b[1;32m    601\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 )\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             raise InvalidParameterError(\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0;34mf\"The {param_name!r} parameter of {caller_name} must be\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;34mf\" {constraints_str}. Got {param_val!r} instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'every', 'except', 'beforehand', 'until', 'seems', 'out', 'though', 'un', 'we', 'will', 'besides', 'full', 'sixty', 'behind', 'herself', 'others', 'yet', 'thereby', 'per', 'thereupon', 'sometimes', 'etc', 'therein', 'nowhere', 'amoungst', 'again', 'much', 'now', 'thus', 'mostly', 'sometime', 'anywhere', 'under', 'hundred', 'other', 'call', 'beside', 'get', 'hence', 'yours', 'would', 'somewhere', 'towards', 'even', 'fire', 'he', 'must', 'cry', 'further', 'up', 'than', 'mine', 'forty', 'thru', 'found', 'hereby', 'yourselves', 'because', 'next', 'whereby', 'con', 'could', 'front', 'their', 'wherever', 'ie', 'two', 'everyone', 'his', 'seem', 'yourself', 'among', 'top', 'whereupon', 'always', 'please', 'himself', 'the', 'while', 'whoever', 'were', 'us', 'into', 'too', 'meanwhile', 'down', 'part', 'empty', 'describe', 'noone', 'from', 'them', 'have', 'such', 'over', 'on', 'sincere', 'neither', 'elsewhere', 'can', 'our', 'made', 'thence', 'former', 'show', 'inc', 'amongst', 'after', 'hereafter', 'has', 'wherein', 'been', 'of', 'to', 'by', 'rather', 'but', 'seeming', 'whole', 'system', 'had', 'four', 'either', 'becoming', 'namely', 'that', 'is', 'here', 'against', 'are', 'anyhow', 'ever', 'without', 'three', 'there', 'least', 'although', 'my', 'about', 'less', 'being', 'de', 'any', 'may', 'whose', 'several', 'through', 'name..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a9S9g9Ty2GwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using SVM and tf-idf"
      ],
      "metadata": {
        "id": "KffCdP43R6wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(df['text'], df['result'], test_size=0.2, random_state=42)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
        "x_train_tfidf = vectorizer.fit_transform(x_train)\n",
        "x_val_tfidf = vectorizer.transform(x_val)\n",
        "\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "ridge_model = RidgeClassifier()\n",
        "ridge_model.fit(x_train_tfidf, y_train)\n",
        "y_pred_ridge = ridge_model.predict(x_val_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_ridge = accuracy_score(y_val, y_pred_ridge)\n",
        "print(f'Validation Accuracy (Ridge Classifier): {accuracy_ridge}')\n",
        "print(\"Classification Report (Ridge Classifier):\")\n",
        "print(classification_report(y_val, y_pred_ridge,digits=6))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvfqwIhzR9BG",
        "outputId": "167c49da-1615-4211-eebf-fbe3455a2155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy (Ridge Classifier): 0.8416815742397138\n",
            "Classification Report (Ridge Classifier):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.868597  0.767717  0.815047       508\n",
            "           1   0.823617  0.903279  0.861611       610\n",
            "\n",
            "    accuracy                       0.841682      1118\n",
            "   macro avg   0.846107  0.835498  0.838329      1118\n",
            "weighted avg   0.844055  0.841682  0.840453      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ...\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Classification Report (SVM):\")\n",
        "print(classification_report(y_val, y_pred,digits=6))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NjF-a1oR9xR",
        "outputId": "d7a60e53-ca5f-4b65-8502-c33f16292837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (SVM):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.845992  0.789370  0.816701       508\n",
            "           1   0.833851  0.880328  0.856459       610\n",
            "\n",
            "    accuracy                       0.838998      1118\n",
            "   macro avg   0.839921  0.834849  0.836580      1118\n",
            "weighted avg   0.839367  0.838998  0.838394      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Random Forest and tf-idf"
      ],
      "metadata": {
        "id": "bb8JUcMWTS_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(x_train_tfidf, y_train)\n",
        "\n",
        "# Predictions on validation set\n",
        "y_pred_rf = rf_model.predict(x_val_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_rf = accuracy_score(y_val, y_pred_rf)\n",
        "print(f'Validation Accuracy (Random Forest): {accuracy_rf}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MDwrGuiSVjH",
        "outputId": "4d184376-16da-4eb5-ed32-82be88b3f916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy (Random Forest): 0.8112701252236136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report (Random Forest):\")\n",
        "print(classification_report(y_val, y_pred_rf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU-IYPE_TY13",
        "outputId": "f75bf430-c4db-4cba-af75-ad5e44e9085f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.68      0.76       508\n",
            "           1       0.77      0.92      0.84       610\n",
            "\n",
            "    accuracy                           0.81      1118\n",
            "   macro avg       0.83      0.80      0.80      1118\n",
            "weighted avg       0.82      0.81      0.81      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gHL8-_W4TfvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Naive Bayes and tf-idf"
      ],
      "metadata": {
        "id": "aMLrx2GgToxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Train Multinomial Naive Bayes\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(x_train_tfidf, y_train)\n",
        "\n",
        "# Predictions on validation set\n",
        "y_pred_nb = nb_model.predict(x_val_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_nb = accuracy_score(y_val, y_pred_nb)\n",
        "print(f'Validation Accuracy (Multinomial Naive Bayes): {accuracy_nb}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vni5kAi3TwYe",
        "outputId": "84e376d4-14f4-4f0f-824e-11067e60ec0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy (Multinomial Naive Bayes): 0.8300536672629696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ...\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Classification Report (Multinomial Naive Bayes):\")\n",
        "print(classification_report(y_val, y_pred_nb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsfOOpHdT0BT",
        "outputId": "a5b6bd05-e759-4afc-e5d6-1949953ec24c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Multinomial Naive Bayes):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.70      0.79       508\n",
            "           1       0.79      0.94      0.86       610\n",
            "\n",
            "    accuracy                           0.83      1118\n",
            "   macro avg       0.85      0.82      0.82      1118\n",
            "weighted avg       0.84      0.83      0.83      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hOK9QXdAT38O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Logistic Regression and tf-idf"
      ],
      "metadata": {
        "id": "5Q8PWXgBUYA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train Logistic Regression\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(x_train_tfidf, y_train)\n",
        "\n",
        "# Predictions on validation set\n",
        "y_pred_lr = lr_model.predict(x_val_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_lr = accuracy_score(y_val, y_pred_lr)\n",
        "print(f'Validation Accuracy (Logistic Regression): {accuracy_lr}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTr5DU8gUiyw",
        "outputId": "3c5e5c1c-9937-4ec1-de7b-711461c8be5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy (Logistic Regression): 0.8416815742397138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report (Logistic Regression):\")\n",
        "print(classification_report(y_val, y_pred_lr,digits=6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVrfECgcUm81",
        "outputId": "d88f9ff2-d81b-4b51-8ab0-039ec49b9d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Logistic Regression):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.842650  0.801181  0.821393       508\n",
            "           1   0.840945  0.875410  0.857831       610\n",
            "\n",
            "    accuracy                       0.841682      1118\n",
            "   macro avg   0.841797  0.838295  0.839612      1118\n",
            "weighted avg   0.841720  0.841682  0.841274      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vtg_1KKmUqgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using ML and word2vec\n"
      ],
      "metadata": {
        "id": "dMv9hio5VrUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from gensim.models import Word2Vec\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text using Word2Vec\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Train Word2Vec model\n",
        "sentences = [text.split() for text in df['text']]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Generate Word2Vec embeddings for each text\n",
        "embeddings = np.array([np.mean([word2vec_model.wv[word] for word in text.split() if word in word2vec_model.wv] or [np.zeros(word2vec_model.vector_size)], axis=0) for text in df['text']])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(embeddings, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# ...\n",
        "\n",
        "# Train Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(x_train, y_train)\n",
        "\n",
        "# Predictions on validation set\n",
        "y_pred_rf = rf_model.predict(x_val)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Classification Report (Random Forest):\")\n",
        "print(classification_report(y_val, y_pred_rf,digits=6))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLhlYnb6Vw0M",
        "outputId": "dcf2d870-e3e2-4b45-e587-5254bb7ae272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.754491  0.744094  0.749257       508\n",
            "           1   0.789303  0.798361  0.793806       610\n",
            "\n",
            "    accuracy                       0.773703      1118\n",
            "   macro avg   0.771897  0.771228  0.771531      1118\n",
            "weighted avg   0.773485  0.773703  0.773564      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# ...\n",
        "\n",
        "# Train Multinomial Naive Bayes\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(x_train, y_train)\n",
        "\n",
        "# Predictions on validation set\n",
        "y_pred_nb = nb_model.predict(x_val)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Classification Report (Multinomial Naive Bayes):\")\n",
        "print(classification_report(y_val, y_pred_nb))"
      ],
      "metadata": {
        "id": "K_VcjwtJiRXr",
        "outputId": "2d42bb8a-c5bf-4d79-87f6-1639e845eca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-31f5002f9c1f>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Train Multinomial Naive Bayes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Predictions on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_counters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0mcheck_non_negative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MultinomialNB (input X)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[0;34m(X, whom)\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX_min\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# ...\n",
        "\n",
        "# Train Logistic Regression\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(x_train, y_train)\n",
        "\n",
        "# Predictions on validation set\n",
        "y_pred_lr = lr_model.predict(x_val)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Classification Report (Logistic Regression):\")\n",
        "print(classification_report(y_val, y_pred_lr,digits=6))"
      ],
      "metadata": {
        "id": "Cw07a5aei8U1",
        "outputId": "a359c57d-1ff3-4f63-b915-39928045ddef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Logistic Regression):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.685345  0.625984  0.654321       508\n",
            "           1   0.709480  0.760656  0.734177       610\n",
            "\n",
            "    accuracy                       0.699463      1118\n",
            "   macro avg   0.697412  0.693320  0.694249      1118\n",
            "weighted avg   0.698513  0.699463  0.697892      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n",
        "ridge_model = RidgeClassifier()\n",
        "ridge_model.fit(x_train, y_train)\n",
        "y_pred_ridge = ridge_model.predict(x_val)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_ridge = accuracy_score(y_val, y_pred_ridge)\n",
        "print(f'Validation Accuracy (Ridge Classifier): {accuracy_ridge}')\n",
        "print(\"Classification Report (Ridge Classifier):\")\n",
        "print(classification_report(y_val, y_pred_ridge,digits=6))"
      ],
      "metadata": {
        "id": "pfoy0ujgjtYl",
        "outputId": "94cafefe-d660-4ae1-84b6-49f7ba740aa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy (Ridge Classifier): 0.725402504472272\n",
            "Classification Report (Ridge Classifier):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.708075  0.673228  0.690212       508\n",
            "           1   0.738583  0.768852  0.753414       610\n",
            "\n",
            "    accuracy                       0.725403      1118\n",
            "   macro avg   0.723329  0.721040  0.721813      1118\n",
            "weighted avg   0.724720  0.725403  0.724696      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Train your Word2Vec model\n",
        "sentences = [text.split() for text in df['text']]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Convert text to Word2Vec embeddings\n",
        "embedding_dim = 100  # Should match the Word2Vec vector_size\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        try:\n",
        "            embedding_matrix[i] = word2vec_model.wv[word]\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use different classifiers\n",
        "\n",
        "## SVM\n",
        "\n"
      ],
      "metadata": {
        "id": "W4flXJc5Vysb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58b4b4ad-9425-4376-fc3b-fe729172afd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(x_train, y_train)\n",
        "y_pred_rf = rf_model.predict(x_val)\n",
        "accuracy_rf = accuracy_score(y_val, y_pred_rf)\n",
        "print(f'Validation Accuracy (Random Forest): {accuracy_rf}')\n",
        "print(\"Classification Report (Random Forest):\")\n",
        "print(classification_report(y_val, y_pred_rf,digits=6))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZydECk_XX0Ns",
        "outputId": "294235c1-77d1-40a0-c1e1-b329e15eceab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy (Random Forest): 0.8050089445438283\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.826577  0.722441  0.771008       508\n",
            "           1   0.790801  0.873770  0.830218       610\n",
            "\n",
            "    accuracy                       0.805009      1118\n",
            "   macro avg   0.808689  0.798106  0.800613      1118\n",
            "weighted avg   0.807057  0.805009  0.803314      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multinomial Naive Bayes (Note: This classifier is typically used with TF-IDF, not Word2Vec)\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(x_train, y_train)\n",
        "y_pred_nb = nb_model.predict(x_val)\n",
        "accuracy_nb = accuracy_score(y_val, y_pred_nb)\n",
        "print(f'Validation Accuracy (Multinomial Naive Bayes): {accuracy_nb}')\n",
        "print(\"Classification Report (Multinomial Naive Bayes):\")\n",
        "print(classification_report(y_val, y_pred_nb,digits=6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KB1dxzdYXgd",
        "outputId": "1ff4d0e3-cee5-4d68-8ffe-d0a6b32b1604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy (Multinomial Naive Bayes): 0.5840787119856887\n",
            "Classification Report (Multinomial Naive Bayes):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.570957  0.340551  0.426634       508\n",
            "           1   0.588957  0.786885  0.673684       610\n",
            "\n",
            "    accuracy                       0.584079      1118\n",
            "   macro avg   0.579957  0.563718  0.550159      1118\n",
            "weighted avg   0.580778  0.584079  0.561429      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(x_train, y_train)\n",
        "y_pred_lr = lr_model.predict(x_val)\n",
        "accuracy_lr = accuracy_score(y_val, y_pred_lr)\n",
        "print(f'Validation Accuracy (Logistic Regression): {accuracy_lr}')\n",
        "print(\"Classification Report (Logistic Regression):\")\n",
        "print(classification_report(y_val, y_pred_lr,digits=6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF212hXQZURq",
        "outputId": "86185a3d-b948-4d77-92cd-9afe43b0d569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy (Logistic Regression): 0.7567084078711985\n",
            "Classification Report (Logistic Regression):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.822404  0.592520  0.688787       508\n",
            "           1   0.724734  0.893443  0.800294       610\n",
            "\n",
            "    accuracy                       0.756708      1118\n",
            "   macro avg   0.773569  0.742981  0.744540      1118\n",
            "weighted avg   0.769114  0.756708  0.749627      1118\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Using ridge classifier"
      ],
      "metadata": {
        "id": "HxffvmWwaVdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Train your Word2Vec model\n",
        "sentences = [text.split() for text in df['text']]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Convert text to Word2Vec embeddings\n",
        "embedding_dim = 100  # Should match the Word2Vec vector_size\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        try:\n",
        "            embedding_matrix[i] = word2vec_model.wv[word]\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use Ridge Classifier\n",
        "ridge_model = RidgeClassifier()\n",
        "ridge_model.fit(x_train, y_train)\n",
        "y_pred_ridge = ridge_model.predict(x_val)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_ridge = accuracy_score(y_val, y_pred_ridge)\n",
        "print(f'Validation Accuracy (Ridge Classifier): {accuracy_ridge}')\n",
        "print(\"Classification Report (Ridge Classifier):\")\n",
        "print(classification_report(y_val, y_pred_ridge,digits=6))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QphQl-BFayHF",
        "outputId": "67e3393c-fd61-4551-ea9f-2e4e9090d0ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n",
            "Validation Accuracy (Ridge Classifier): 0.7558139534883721\n",
            "Classification Report (Ridge Classifier):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.885246  0.531496  0.664207       508\n",
            "           1   0.707257  0.942623  0.808152       610\n",
            "\n",
            "    accuracy                       0.755814      1118\n",
            "   macro avg   0.796251  0.737060  0.736179      1118\n",
            "weighted avg   0.788132  0.755814  0.742746      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZjJzfbPazYk",
        "outputId": "ec406780-d64b-4810-fc68-24d0093830fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.23.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199770 sha256=7bebffa9a1b9334514f2c469df690e6e0aa8559926c8e2c7f81800c2fcb017d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using fasttext and ML"
      ],
      "metadata": {
        "id": "_sSg5Y8BbwKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import fasttext\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Format data for FastText training\n",
        "fasttext_data = []\n",
        "for text, label in zip(df['text'], df['result']):\n",
        "    fasttext_data.append(f\"__label__{label} {text}\")\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(fasttext_data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save training data to a file\n",
        "train_file_path = 'fasttext_train.txt'\n",
        "with open(train_file_path, 'w', encoding='utf-8') as f:\n",
        "    for item in x_train:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "\n",
        "# Train FastText model\n",
        "model = fasttext.train_supervised(input=train_file_path, epoch=10, wordNgrams=2, minCount=1, lr=0.1, dim=100)\n",
        "\n",
        "# Convert text to FastText embeddings\n",
        "embedding_dim = 100  # Should match the FastText dimension used in training\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        try:\n",
        "            embedding_matrix[i] = model.get_word_vector(word)\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use different classifiers\n",
        "\n",
        "## RandomForest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(x_train, y_train)\n",
        "y_pred_rf = rf_model.predict(x_val)\n",
        "accuracy_rf = accuracy_score(y_val, y_pred_rf)\n",
        "print(f'Validation Accuracy (Random Forest): {accuracy_rf}')\n",
        "print(\"Classification Report (Random Forest):\")\n",
        "print(classification_report(y_val, y_pred_rf))\n",
        "\n",
        "\n",
        "\n",
        "## Ridge Classifier\n",
        "ridge_model = RidgeClassifier()\n",
        "ridge_model.fit(x_train, y_train)\n",
        "y_pred_ridge = ridge_model.predict(x_val)\n",
        "accuracy_ridge = accuracy_score(y_val, y_pred_ridge)\n",
        "print(f'Validation Accuracy (Ridge Classifier): {accuracy_ridge}')\n",
        "print(\"Classification Report (Ridge Classifier):\")\n",
        "print(classification_report(y_val, y_pred_ridge))\n",
        "\n",
        "## Multinomial Naive Bayes\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(x_train, y_train)\n",
        "y_pred_nb = nb_model.predict(x_val)\n",
        "accuracy_nb = accuracy_score(y_val, y_pred_nb)\n",
        "print(f'Validation Accuracy (Multinomial Naive Bayes): {accuracy_nb}')\n",
        "print(\"Classification Report (Multinomial Naive Bayes):\")\n",
        "print(classification_report(y_val, y_pred_nb))\n",
        "\n",
        "## Logistic Regression\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(x_train, y_train)\n",
        "y_pred_lr = lr_model.predict(x_val)\n",
        "accuracy_lr = accuracy_score(y_val, y_pred_lr)\n",
        "print(f'Validation Accuracy (Logistic Regression): {accuracy_lr}')\n",
        "print(\"Classification Report (Logistic Regression):\")\n",
        "print(classification_report(y_val, y_pred_lr))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3zqBU3wbvgz",
        "outputId": "3bc1f6d1-9194-4bef-f87d-5e64b1d5531f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6236 unique tokens.\n",
            "Validation Accuracy (Random Forest): 0.8050089445438283\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.72      0.77       508\n",
            "           1       0.79      0.87      0.83       610\n",
            "\n",
            "    accuracy                           0.81      1118\n",
            "   macro avg       0.81      0.80      0.80      1118\n",
            "weighted avg       0.81      0.81      0.80      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using glove and ML"
      ],
      "metadata": {
        "id": "gps5PZXlh8BC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Choose based on your dataset\n",
        "maxlen = 100  # Choose based on your dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Load pre-trained GloVe embeddings\n",
        "glove_file_path = '/content/glove.6B.100d.txt'  # Replace with the path to your GloVe file\n",
        "embedding_dim = 100  # Should match the GloVe embedding dimension\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(glove_file_path, encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "# Convert text to GloVe embeddings\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use different classifiers\n",
        "\n",
        "## RandomForest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(x_train, y_train)\n",
        "y_pred_rf = rf_model.predict(x_val)\n",
        "accuracy_rf = accuracy_score(y_val, y_pred_rf)\n",
        "print(f'Validation Accuracy (Random Forest): {accuracy_rf}')\n",
        "print(\"Classification Report (Random Forest):\")\n",
        "print(classification_report(y_val, y_pred_rf,digits=6))\n",
        "\n",
        "\n",
        "\n",
        "## Ridge Classifier\n",
        "ridge_model = RidgeClassifier()\n",
        "ridge_model.fit(x_train, y_train)\n",
        "y_pred_ridge = ridge_model.predict(x_val)\n",
        "accuracy_ridge = accuracy_score(y_val, y_pred_ridge)\n",
        "print(f'Validation Accuracy (Ridge Classifier): {accuracy_ridge}')\n",
        "print(\"Classification Report (Ridge Classifier):\")\n",
        "print(classification_report(y_val, y_pred_ridge,digits=6))\n",
        "\n",
        "## Multinomial Naive Bayes (Note: This classifier is typically used with TF-IDF, not GloVe)\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(x_train, y_train)\n",
        "y_pred_nb = nb_model.predict(x_val)\n",
        "accuracy_nb = accuracy_score(y_val, y_pred_nb)\n",
        "print(f'Validation Accuracy (Multinomial Naive Bayes): {accuracy_nb}')\n",
        "print(\"Classification Report (Multinomial Naive Bayes):\")\n",
        "print(classification_report(y_val, y_pred_nb,digits=6))\n",
        "\n",
        "## Logistic Regression\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(x_train, y_train)\n",
        "y_pred_lr = lr_model.predict(x_val)\n",
        "accuracy_lr = accuracy_score(y_val, y_pred_lr)\n",
        "print(f'Validation Accuracy (Logistic Regression): {accuracy_lr}')\n",
        "print(\"Classification Report (Logistic Regression):\")\n",
        "print(classification_report(y_val, y_pred_lr,digits=6))\n"
      ],
      "metadata": {
        "id": "pS-yX0w-byHo",
        "outputId": "c168e80f-abc1-4ad2-c046-71dcdbe0695e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6236 unique tokens.\n",
            "Validation Accuracy (Random Forest): 0.8050089445438283\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.826577  0.722441  0.771008       508\n",
            "           1   0.790801  0.873770  0.830218       610\n",
            "\n",
            "    accuracy                       0.805009      1118\n",
            "   macro avg   0.808689  0.798106  0.800613      1118\n",
            "weighted avg   0.807057  0.805009  0.803314      1118\n",
            "\n",
            "Validation Accuracy (Ridge Classifier): 0.7558139534883721\n",
            "Classification Report (Ridge Classifier):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.885246  0.531496  0.664207       508\n",
            "           1   0.707257  0.942623  0.808152       610\n",
            "\n",
            "    accuracy                       0.755814      1118\n",
            "   macro avg   0.796251  0.737060  0.736179      1118\n",
            "weighted avg   0.788132  0.755814  0.742746      1118\n",
            "\n",
            "Validation Accuracy (Multinomial Naive Bayes): 0.5840787119856887\n",
            "Classification Report (Multinomial Naive Bayes):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.570957  0.340551  0.426634       508\n",
            "           1   0.588957  0.786885  0.673684       610\n",
            "\n",
            "    accuracy                       0.584079      1118\n",
            "   macro avg   0.579957  0.563718  0.550159      1118\n",
            "weighted avg   0.580778  0.584079  0.561429      1118\n",
            "\n",
            "Validation Accuracy (Logistic Regression): 0.7567084078711985\n",
            "Classification Report (Logistic Regression):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.822404  0.592520  0.688787       508\n",
            "           1   0.724734  0.893443  0.800294       610\n",
            "\n",
            "    accuracy                       0.756708      1118\n",
            "   macro avg   0.773569  0.742981  0.744540      1118\n",
            "weighted avg   0.769114  0.756708  0.749627      1118\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Fasttext and ML"
      ],
      "metadata": {
        "id": "XU3hDXV4ksy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from gensim.models import FastText\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Train FastText model\n",
        "sentences = [text.split() for text in df['text']]\n",
        "fasttext_model = FastText(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Prepare feature matrix\n",
        "X = np.array([np.mean([fasttext_model.wv[word] for word in text.split() if word in fasttext_model.wv] or [np.zeros(fasttext_model.vector_size)], axis=0) for text in df['text']])\n",
        "y = df['result'].values\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Machine Learning Models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000)\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "for name, model in models.items():\n",
        "    # Use a pipeline if normalization is required\n",
        "    # model = Pipeline([('scaler', StandardScaler()), ('classifier', model)])\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Classification report\n",
        "    print(f\"Classification Report ({name}):\")\n",
        "    print(classification_report(y_val, y_pred,digits=6))\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    print(f'Validation Accuracy ({name}): {accuracy}')\n"
      ],
      "metadata": {
        "id": "1cgilNPVksd4",
        "outputId": "1ec8bf24-17bf-4874-e4f6-84844e4de8b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.698444  0.706693  0.702544       508\n",
            "           1   0.753311  0.745902  0.749588       610\n",
            "\n",
            "    accuracy                       0.728086      1118\n",
            "   macro avg   0.725877  0.726297  0.726066      1118\n",
            "weighted avg   0.728380  0.728086  0.728212      1118\n",
            "\n",
            "Validation Accuracy (Random Forest): 0.7280858676207513\n",
            "Classification Report (Logistic Regression):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.662953  0.468504  0.549020       508\n",
            "           1   0.644269  0.801639  0.714390       610\n",
            "\n",
            "    accuracy                       0.650268      1118\n",
            "   macro avg   0.653611  0.635072  0.631705      1118\n",
            "weighted avg   0.652758  0.650268  0.639249      1118\n",
            "\n",
            "Validation Accuracy (Logistic Regression): 0.6502683363148479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n",
        "ridge_model = RidgeClassifier()\n",
        "ridge_model.fit(X_train, y_train)\n",
        "y_pred_ridge = ridge_model.predict(X_val)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_ridge = accuracy_score(y_val, y_pred_ridge)\n",
        "print(f'Validation Accuracy (Ridge Classifier): {accuracy_ridge}')\n",
        "print(\"Classification Report (Ridge Classifier):\")\n",
        "print(classification_report(y_val, y_pred_ridge,digits=6))"
      ],
      "metadata": {
        "id": "HXZQBXhkiF4p",
        "outputId": "6c595e20-cbf1-4a70-ee4b-463a7e2cde80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy (Ridge Classifier): 0.6762075134168157\n",
            "Classification Report (Ridge Classifier):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.672986  0.559055  0.610753       508\n",
            "           1   0.678161  0.773770  0.722818       610\n",
            "\n",
            "    accuracy                       0.676208      1118\n",
            "   macro avg   0.675573  0.666413  0.666785      1118\n",
            "weighted avg   0.675809  0.676208  0.671897      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Glove and ML"
      ],
      "metadata": {
        "id": "HSXVc2tHnV7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame named 'df' with 'text' and 'result' columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Download GloVe embeddings (choose the appropriate dimension, e.g., 50, 100, 200, 300)\n",
        "glove_file = 'glove.6B.100d.txt'\n",
        "glove_w2v_file = 'glove.6B.100d.w2v.txt'\n",
        "\n",
        "# Convert GloVe format to Word2Vec format\n",
        "glove2word2vec(glove_file, glove_w2v_file)\n",
        "\n",
        "# Load the GloVe Word2Vec model\n",
        "glove_model = KeyedVectors.load_word2vec_format(glove_w2v_file, binary=False)\n",
        "\n",
        "# Create TF-IDF matrix\n",
        "vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(df['text']).toarray()\n",
        "\n",
        "# Generate document embeddings using GloVe\n",
        "embeddings = []\n",
        "for text in df['text']:\n",
        "    words = text.split()\n",
        "    vectors = [glove_model[word] for word in words if word in glove_model]\n",
        "    if vectors:\n",
        "        embeddings.append(np.mean(vectors, axis=0))\n",
        "    else:\n",
        "        embeddings.append(np.zeros(glove_model.vector_size))\n",
        "\n",
        "data = np.array(embeddings)\n",
        "labels = df['result'].values\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# ...\n",
        "\n"
      ],
      "metadata": {
        "id": "rSVboYW6mXwi",
        "outputId": "7acaac0f-aa97-4a2f-fa96-6c783c16eff6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-f43037e9a112>:18: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
            "  glove2word2vec(glove_file, glove_w2v_file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.77      0.82       508\n",
            "           1       0.82      0.90      0.86       610\n",
            "\n",
            "    accuracy                           0.84      1118\n",
            "   macro avg       0.85      0.84      0.84      1118\n",
            "weighted avg       0.84      0.84      0.84      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(x_train, y_train)\n",
        "\n",
        "# Predictions on validation set\n",
        "y_pred_rf = rf_model.predict(x_val)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Classification Report (Random Forest):\")\n",
        "print(classification_report(y_val, y_pred_rf,digits=6))"
      ],
      "metadata": {
        "id": "aKrIvFnQnd8n",
        "outputId": "f16acaa0-3733-4429-d02d-01b210a87e26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.866962  0.769685  0.815433       508\n",
            "           1   0.824588  0.901639  0.861394       610\n",
            "\n",
            "    accuracy                       0.841682      1118\n",
            "   macro avg   0.845775  0.835662  0.838413      1118\n",
            "weighted avg   0.843842  0.841682  0.840510      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_model = RidgeClassifier()\n",
        "ridge_model.fit(x_train, y_train)\n",
        "y_pred_ridge = ridge_model.predict(x_val)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_ridge = accuracy_score(y_val, y_pred_ridge)\n",
        "print(f'Validation Accuracy (Ridge Classifier): {accuracy_ridge}')\n",
        "print(\"Classification Report (Ridge Classifier):\")\n",
        "print(classification_report(y_val, y_pred_ridge,digits=6))"
      ],
      "metadata": {
        "id": "B_DuveYXn08L",
        "outputId": "6395fbc3-b304-4370-86e7-c3ffa0497e43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy (Ridge Classifier): 0.8461538461538461\n",
            "Classification Report (Ridge Classifier):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.862069  0.787402  0.823045       508\n",
            "           1   0.834862  0.895082  0.863924       610\n",
            "\n",
            "    accuracy                       0.846154      1118\n",
            "   macro avg   0.848466  0.841242  0.843485      1118\n",
            "weighted avg   0.847225  0.846154  0.845349      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(x_train, y_train)\n",
        "y_pred_lr = lr_model.predict(x_val)\n",
        "accuracy_lr = accuracy_score(y_val, y_pred_lr)\n",
        "print(f'Validation Accuracy (Logistic Regression): {accuracy_lr}')\n",
        "print(\"Classification Report (Logistic Regression):\")\n",
        "print(classification_report(y_val, y_pred_lr,digits=6))"
      ],
      "metadata": {
        "id": "R_d6CIxeoTgt",
        "outputId": "bd12adf6-b9c9-48b0-fb31-2a7d0465d465",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy (Logistic Regression): 0.8497316636851521\n",
            "Classification Report (Logistic Regression):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.858650  0.801181  0.828921       508\n",
            "           1   0.843168  0.890164  0.866029       610\n",
            "\n",
            "    accuracy                       0.849732      1118\n",
            "   macro avg   0.850909  0.845673  0.847475      1118\n",
            "weighted avg   0.850202  0.849732  0.849167      1118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lZDcDH_zox_H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}